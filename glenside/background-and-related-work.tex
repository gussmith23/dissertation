\section{Background and Related Work}


\g is designed to help target
  tensor hardware accelerators and
  builds on past work in
  tensor IRs and term rewriting.

% Here,
%   we give some background
%   on equality saturation,
%   tensor IRs, 
%   and the process
%   of mapping these programs
%   to hardware.


\subsection{Machine Learning Accelerators}

A variety of accelerators~\cite{
    jouppi2017tpu, chen2016eyeriss, moreau2018vta, markidis2018tensorcore, nvdla}
  have been developed 
  to provide efficient implementations
  of tensor operators for ML applications.
These devices accelerate tensor operators 
  through hardware parallelism, 
  simultaneously applying related operations
  across many tensors in the accelerator's memory (which are often laid out according to custom rules that facilitate hardware optimization).
  %in the accelerator's memory,
  %often laid out according to custom rules
  %that facilitate hardware optimization.
Tensor program compilers must translate
  expensive application code fragments
  down to accelerator invocations that
  adhere to these layout rules,
  which often involves both
  (a) higher-level transformations like
  tensor reshaping to match accelerator size bounds and
  loop unrolling to expose optimization opportunities, and
  (b) lower-level transformations like
  operator fusion and \tcd{im2col}
  to match accelerator calling conventions and
  even implement different operations
  using the same accelerator,
  e.g., on systolic arrays~\cite{im2col, jia2014semantic}.
  
% Hence, it becomes the task of the compiler 
%   to ensure that data 
%   can be efficiently offloaded 
%   to these accelerators 
%   in the expected format,
%   sometimes requiring higher-level program transformations
%   to avoid redundant transformations.
% \hl{Maybe this isn't the place for im2col?} Additionally, 
%   certain layout transformations 
%   can be used to implement different operations using the same accelerator,
%   as in the above-mentioned \tcd{im2col} example.

\subsection{Tensor IRs and Compilers}

%Machine learning workloads
%  are generally viewed
%  as a sequence of 
%  of \textit{tensor kernel} invocations,
%  where tensor kernels
%  are large operations
%  over multidimensional arrays (tensors)
%  such as 2D convolution
%  or dense matrix multiplication.
%Machine learning frameworks
%  (such as TensorFlow \cite{tensorflow}
%    and PyTorch \cite{pytorch})
%  and machine learning compilers
%  (such as TVM \cite{chen2018tvm})
%  can optimize
%  machine learning workloads
%  at a number of levels,
%  which often result
%  in each framework
%  having a number of different IRs.
%TVM, for example,
%  can optimize machine learning workloads
%  at a high-level
%  \hl{example?}
%  using its high-level representation
%  Relay \cite{relay},
%  but
%  low-level optimizations
%  such as loop blocking and reordering
%  must be done 
%  in its lower-level IRs,
%  TIR and TE.

% \hl{not mentioning hand-optimized kernels like CuDNN,
% but maybe that's OK for focus / space}

Tensor compilers for ML and HPC applications strive
  to balance clear, high-level operator semantics
  and support for the low-level optimizations
  necessary to target specialized accelerators.
Halide~\cite{halide}
  achieves this balance by separating
  operator \textit{specifications} (what is computed) from
  \textit{schedules} (how, when, and where
  each output element is generated).
This style of separation has proven
  highly effective across both
  application domains and hardware targets;
  numerous compilers including TVM~\cite{chen2018tvm},
  FireIron~\cite{hagedorn2020fireiron},
  LIFT~\cite{lift}, and Accelerate~\cite{accelerate}
  follow variations of this strategy.
  
The specification/schedule separation approach
  allows the same high-level program (specification)
  to be flexibly optimized for and mapped to
  different hardware targets by applying different schedules.
From this perspective,
  schedules represent different rewriting strategies
  to explore various loop ordering and memory layouts;
  in LIFT and Accelerate these
  take the form of functional combinators
  closely related to \g's approach.
As in classic term rewriting,
  experts must often carefully craft
  schedules for each target to achieve
  the best performance and mitigate
  phase ordering challenges~\cite{phase-ordering},
  though recent projects have produced promising results
  in automatic scheduling~\cite{
    chen2018autotvm, zheng2020ansor, anderson2020learning}.

Other tensor IRs like
  TACO~\cite{taco}, Keops~\cite{keops},
  and COMET~\cite{tian2021highperformance}
  rely on \textit{index notation}\footnote{
    Index notation is closely related to
    ``Einstein notation'' where reduction
    indices are implicit.}
  to concisely express tensor operators
  and simplify optimization by
  uniformly representing
  per-output-element computations.
These approaches also rely on
  rewriting passes to generate
  kernel implementations specialized to
  tensor sparsity / density,
  operator combinations arising in
  the source application, and
  details of the target hardware.
In Section~\ref{sec:matmul} we discuss
  some of the tradeoffs of these approaches
  with respect to other rewriting strategies.
 
Finally, polyhedral compilers~\cite{polyhedral-survey}
  like Tensor Comprehensions~\cite{vasilache2018tensor}
  and Tiramisu~\cite{tiramisu}
  optimize loop-filled programs
  by modeling loop nests as polyhedra
  and applying geometric transformations.
The polyhedral approach exploits
  regular loop structure,
  but is also restricted
  to geometrically affine transformations.
In contrast, term rewriting is
  neither guided nor restricted by
  geometric constraints, making
  these approaches broadly complementary.


% Accelerators and parallel architectures 
%   present many opportunities 
%   for tuning and improving tensor operator implementations for the best performance.
% Many systems, 
%   including deep learning frameworks like PyTorch~\cite{pytorch},
%   make extensive use of hand-optimized tensor operator implementations
%   (such as those in the CuDNN library~\cite{chetlur2014cudnn}).
% While these hand-optimized implementations
%   achieve strong performance,
%   various systems, including \g,
%   have been proposed to automatically
%   generate optimized tensor operators
%   to reduce development time,
%   support varied hardware platforms,
%   and avoid reliance on a small number of experts.
% These tensor compilers 
%   rely on specifying tensor computations 
%   in a form that allows for
%   conveniently exploring options for optimizations, 
%   such as by changing loop orderings
%   to best utilize caches.

% \hl{Surely there's a better summary/gloss here.}
% Many tensor operator compilers 
%   use index notation
%   to specify tensor operators
%   (defining how each element of the output
%   is to be computed
%   based on corresponding elements 
%   of the input).
% TACO~\cite{taco} uses index notation 
%   for automatically generating and optimzing both sparse and dense tensor computations,
%   permitting particular dimensions 
%   to be specified as dense or sparse,
%   introducing various structures for reasoning about how 
%   to iterate over sparse dimensions.
% %Tensor operators in TACO
%   %are described using index notation, 
%   %with particular tensor dimensions 
%   %being either dense or sparse.
% %\cite{taco} initially 
%   %focused on compiling for CPU, 
%   %while subsequent work~\cite{senanayake2020}
%   %has extended TACO to target
%   %GPUs and vector units.
% %  is a compiler notable for its handling
% %  of computations over sparse tensors.
% %\hl{unsure how to draw any connection here, but i feel like i need to at least mention taco}
% Keops~\cite{keops} also uses
%   index notation to describe tensor operations
%   for both dense and spare matrices,
%   but introduces another matrix representation
%   called symbolic matrices, 
%   in which each element of the symbolic matrix
%   is computed from two smaller data matrices.
% Computations on symbolic matrices 
%   can more easily be deployed to GPUs and other accelerators 
%   than sparse computations
%   but require less data movement than dense computations.
% %% I think MLIR isn't relevant in this discussion: IRs built _in_ MLIR might be
% %MLIR~\cite{mlir} is a general-purpose compiler framework intended to facilitate the development of IRs.
% %While MLIR itself does not have a built-in notion of tensorization, it has been used to implement polyhedral IRs and other tensor operator representations.
% COMET~\cite{tian2021highperformance} is another
%   CPU-focused sparse tensor algebra IR,
%   implemented using MLIR~\cite{mlir},
%   that additionally uses information about sparse storage formats 
%   in its representation 
%   to increase temporal and spatial locality.

% Halide~\cite{ragan2013halide}
%   %while not a machine learning compiler
%   has been influential
%   in using a representation
%   that separates
%   an tensor operator's specification (what it computes)
%   from its schedule (what order elements are computed),
%   allowing many optimizations
%   to be described as different schedules
%   for the same specification
%   and facilitating operator fusion.
% As discussed in~\cite{newcomb2020halide-rewrite},
%   Halide applies a term rewriting system
%   for producing optimized code.
%   \hl{Any insight for why Halide is amenable to a rewrite system?}
% TVM~\cite{chen2018tvm}
%   uses a similar specification-schedule split
%   for describing its tensor operators.
% FireIron~\cite{hagedorn2020fireiron} is another system
%   using a split between a computation specification 
%   and its realization
%   (using a decompositions, rather than Halide-style schedules,
%   for scheduling parts in a top-down style).
% \g can be understood
%   as a further refinement
%   on an algorithm's specification,
%   disambiguating
%   how data is accessed
%   from the computation being performed
%   over the data,
%   enabling transformations
%   like mapping to hardware
%   before the program is scheduled.
%   \hl{What does it mean
%   that mapping to hardware
%   is done before the program is scheduled?}

% Like \g, several other systems 
%   use functional representations
%   of tensor operators.
% LIFT~\cite{lift}
%   separates specifications from schedules
%   using combinators like maps and reductions to specify schedules,
%   an idea further explored in~\cite{hagedorn2020func-high-perf}.
% \hl{anything else to say about LIFT? Contrast with Halide/TVM's scheduling languages?}
% Accelerate~\cite{accelerate} 
%   uses functional combinators 
%   to map Haskell code 
%   to GPU programming primitives.
% \hl{Discussion comparing these to \g?}

% The polyhedral representation~\cite{polyhedral-survey} 
%   has also proven useful
%   for expressing tensor operators.
% %Polyhedral compilation~\cite{polyhedral-survey}
%   %is a powerful alternative
%   %to term rewriting-based compilers.
% Polyhedral compilers optimize
%   loop-filled programs
%   by modeling loop nests
%   as polyhedra
%   and transforming them geometrically.
% Tensor Comprehensions~\cite{vasilache2018tensor}
%   and Tiramisu~\cite{tiramisu}
%   are examples of machine learning compilers
%   powered by the polyhedral framework.
% \hl{Need note about deploying to accelerators?}
% \hl{Implications for term rewriting?}
% Multidimensional homomorphisms~\cite{multidimensional-homomorphism} 
%   are another mathematical representation 
%   of tensor operations
%   that has been amenable to automatic parallelization 
%   and allows for hierarchical composition.
%   \hl{any comment in relation to \g?}

%There are at least as many
  %tensor IRs
  %as there are
  %deep learning frameworks,
  %and many frameworks
  %use more than one.
%TVM \cite{chen2018tvm},
  %for example,
  %uses the high-level
  %Relay IR~\cite{relay}
  %for optimizations
  %in the sequencing of tensor kernels,
  %while it uses low-level TIR
  %to optimize kernels themselves.
%\hl{If you're going to complain about TIR not being pure, you need to establish that purity is a requirement before this. Maybe discuss this later}
%Relay and TIR
  %are a perfect example
  %of our impedance mismatch problem:
  %Relay is pure but too high-level,
  %while
  %TIR is low-level but impure.


% Should we talk about BYOC here?
%Recently,
%  the Bring Your Own Codegen
%  (BYOC)
%  framework
%  was merged into TVM \cite{byoc}.
%BYOC
%  is novel,
%  as it allows for the 
%  fast implementation
%  of a full compiler
%  for deep learning models
%  down to custom hardware backends
%  with fairly little additional code,
%  assuming the users
%  have written a codegen
%  for their hardware
%  already.
%BYOC includes
%  a matching framework,
%  for matching
%  user-specified
%  patterns
%  of Relay code.
%As we will see
%  in Section \ref{sec:case-study-tensorization},
%  one of our primary case studies
%  of \g{}
%  is as a pattern matcher
%  for matching such computational patterns---%
%  in fact,
%  it was the original
%  intended use case
%  of \g{}!
%In some sense,
%  the two pattern matchers
%  are incomparable,
%  because,
%  as we've described above,
%  \g{}
%  and Relay
%  operate at different levels
%  of abstraction.
%Yet if we ignore the differences
%  in the levels of abstraction,
%  we can compare
%  the matchers
%  in how they operate.
%In this comparison,
%  \g{} shines through
%  with the help of
%  the \egr{}.
%Because rewrites
%  are so cheap and easy
%  within the \egr{},
%  we can run a huge number
%  of exploratory rewrites,
%  potentially finding places
%  to map in user-defined
%  patterns.
%BYOC,
%  on the other hand,
%  expects the user to provide
%  multiple equivalent patterns
%  to cover the numerous ways
%  their pattern might appear 
%  in the program
%  \hl{(verify this)}.

%Named tensors \cite{chiang2021named}
%  are a primary inspiration
%  for the access pattern data structure
%  at the core of \g.
%%Tensors have a specific layout
%%  in memory,
%%  a hardware detail
%%  that often bubbles up
%%  even to the highest levels
%%  of tensor abstractions.
%%For example,
%%  in both TVM and TensorFlow,
%%  the \tcd{conv2d} operator
%%  requires the user to specify
%%  the layout of the activation tensor---%
%%  \tcd{NCHW} or \tcd{NHWC},
%%  where \tcd{N} is the batch dimension,
%%  \tcd{C} is the channels dimension,
%%  and \tcd{H} and \tcd{W} are the spatial
%%  (height and width)
%%  dimensions.
%%Rather than pass layout information
%%  externally,
%%  named tensors
%%  incorporate dimension names
%%  directly into the tensor abstraction.
%%Named tensors
%%  acknowledge the fact
%%  that
%%  not all dimensions
%%  are made the same,
%%  and that different dimensions
%%  have different functions.
%Named tensors
%  assign names
%  to the dimensions of a tensor,
%  increasing code readability
%  and acknowledging that different dimensions
%  have different functions.
%Though
%  we do not actually adopt
%  this notation
%  in \g{}---%
%  the dimensions of tensors
%  are still just indexed
%  with integers---%
%  this core idea
%  is reflected
%  in the design
%  of access patterns,
%  which encode the fact that
%  some dimensions are
%  for
%  \textit{iterating over,}
%  while others are meant to be 
%  \textit{computed upon.}

%Many machine learning frameworks
  %have an optimization step
  %in which they match program patterns
  %to efficient low-level implementations,
  %whether that be offloading to hardware
  %or calling into a tuned library.
%TVM \cite{chen2018tvm}, 
%% Steve: I would say Latte is not very relevant here
%% It provides AD, but it requires the user to manually implement just about everything else
  %Latte \cite{latte},


%% Steven: Not clear it's relevant to this context
%Norman Ramsey instruction selection~\cite{dias2010instruction-selection}

%\hl{even further back:} apl?~\cite{apl-survey} % probably no

%\subsection{Term Rewriting, Equality Graphs, and Equality Saturation}
\subsection{Term Rewriting and Equality Saturation}

Term rewriting is a classic
  program optimization technique~\cite{baader1998term}
  that relies on iteratively applying
  rewrite rules of the form $\ell \xrightarrow{} r$:
  when part of a program
  matches the pattern $\ell$
  under substitution $\sigma$,
  it is rewritten into $\sigma(r)$.
This approach is ubiquitous,
  frequently used in both mainstream and DSL compilers
  to implement features including preprocessing,
  strength reductions, and
  peephole optimizations~\cite{garavel2018rewrite-context}.
  
Classic term rewriting systems where
  rewrites are applied destructively suffer
  phase ordering problems~\cite{phase-ordering}:
  the order in which rewrites are applied can
  enhance or severely diminish performance.
Recent work has shown how program synthesis
  can help address this challenge in
  peephole optimizers like Halide's
  scalar expression rewriter~\cite{
    newcomb2020halide-rewrite}.
  
Advances in alternate rewriting techniques
  like equality saturation~\cite{tate2009equality}
  also mitigate phase ordering by exploiting
  the e-graph data structure from SMT solvers
  to repeatedly apply all rewrites simultaneously,
  thus obviating rule ordering considerations.
In particular,
  the \tcd{egg} library~\cite{willsey2021egg}
  has been used to develop new synthesis and
  optimization tools across diverse domains~\cite{
    herbie, szalinski, wang2020spores},
  including DSP compiler vectorization~\cite{
    vanhattum2021vectorization} and
  tensor computation graphs~\cite{yang2021equality}.

\g provides the first tensor IR
  amenable to equality saturation by
  introducing access patterns to
  provide pure, higher order tensor
  kernel combinators that support
  rank-polymorphism without the need
  for binding structures like
  anonymous functions or index notation.

% {\color{red} \noindent\rule{8.5cm}{10pt}}

% \g utilizes
%   the power of equality saturation
%   in
%   low-level tensor program
%   transformation---%
%   namely, mapping programs
%   to custom hardware.
  
% Term rewriting
%   is a classic technique
%   in program optimization
%   in which rewrite rules of the form
%   $\ell \xrightarrow{} r$
%   are applied over a program.
% When an expression in the program
%   matches
%   the pattern $\ell$,
%   it is rewritten
%   into the pattern $r$.
% In classic term rewriting systems,
%   the ordering
%   in which rewrites are applied
%   can affect whether optimizations
%   are found,
%   as some rewrites
%   may reverse or muddle
%   the effects
%   of other rewrites.
% Equality saturation
%   is a technique
%   developed to sidestep
%   this 
%   phase ordering problem.
% The technique
%   has recently re-emerged
%   with the development
%   of the \tcd{egg} library
%   \cite{willsey2021egg},
%   and with the successful application
%   of equality saturation
%   to floating point optimization \cite{herbie}
%   and CAD program simplification \cite{szalinski},
%   among other varied fields.

% %A primary feature
% %  of equality saturation
% %  is its ability to
% %  efficiently store
% %  all previously discovered
% %  versions 
% %  of the program
% %  being transformed.
% %The data structure which enables this
% %  is the
% %  \textit{equality graph}
% %  (\egr),
% %  which resembles
% %  a standard program graph (\hl{not sure on words to use here?}),
% %  except
% %  each node in the program
% %  is replaced with a \textit{set} of nodes,
% %  called an \textit{equivalence class}
% %  or \textit{eclass.}
% %All nodes within an eclass
% %  are equivalent, and thus 
% %  can be substituted for one another
% %  to produce an equivalent program.
% %This property is only possible
% %  when the language used
% %  within the \egr
% %  is pure,
% %  which thus becomes a requirement
% %  for any language
% %  we'd like to transform
% %  with equality saturation.
  
% Diospyros \cite{vanhattum2021vectorization}
%   is a vectorizing compiler
%   for DSPs
%   which uses equality saturation
%   to match program fragments
%   to hardware primitives.
% Diospyros
%   starts from scalar programs
%   and, via symbolic evaluation
%   and equality saturation,
%   lifts them to vectorized
%   expressions,
%   while \g starts
%   from a higher level of abstraction
%   and searches downwards.
  
% SPORES \cite{wang2020spores}
%   and Tensat \cite{yang2021equality}
%   utilize equality saturation
%   to optimize
%   linear algebra expressions
%   and tensor computation graphs,
%   respectively.
% While both
%   show the potential
%   of equality saturation
%   for tensor program transformation,
%   both of their IRs
%   are too high-level
%   for the task of mapping to hardware.

% TODO mention somewhere in here that lambdas (which a lot of langs will use) are a problem for first-order rewrite systems
