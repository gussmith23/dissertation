% TODOs:
% https://docs.google.com/document/d/1ripAPTkGUNGZVeOzdDt2ntLmCOLEM9fb_q4HeZa9NQc/edit?usp=sharing

\documentclass[prologue, dvipsnames, sigplan, screen, review, anonymous]{acmart}
\setcopyright{none}
\settopmatter{printacmref=false}

\usepackage{ragged2e}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{syntax} % for grammar
\usepackage{tabularx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{soul} % for \hl
\usepackage{xspace}
%\usepackage{courier}
\usepackage{float}
\usepackage{enumitem}

\usepackage[T1]{fontenc}
\usepackage{inconsolata}


\lstset{columns=fullflexible}
\lstset{basicstyle=\small\ttfamily,breaklines=true,keepspaces=true,}


%\newcommand{\g}{Glenside\xspace}
\newcommand{\g}{Lakeroad\xspace}
\newcommand{\hwsw}{hardware--software}
\newcommand{\accesspatternshape}[2]{$($$\left( #1 \right)$, $\left( #2 \right)$$)$}
%\newcommand{\accesspatternshape}[2]{$( ( #1 ) , ( #2 ) )$}
\newcommand{\itc}{\texttt{im2col}\xspace}
\newcommand{\ctd}{\texttt{conv2d}\xspace}
\newcommand{\egr}{egraph\xspace}
\newcommand{\egrs}{\egr{}s\xspace}
\newcommand{\Egr}{Egraph\xspace}
\newcommand{\Egrs}{\Egr{}s\xspace}

% "codeword" ie im2col and conv2d
\newcommand{\tcd}[1]{\texttt{#1}}
\newcommand{\mcd}[1]{\mathrm{\tcd{#1}}}


% one  line, access  patterns, no double "representation"
%\title{Access Patterns: A Pure, Low-level Tensor IR}
%\title{Pure, Low-Level Tensor Programs via Access Patterns}
\title{Pure, Low-Level Tensor Program Rewriting \\
via Access Patterns}

%\subtitle{(\textit{representation pearl})}
%\subtitle{A Representation Pearl}
%\subtitle{(A Representation Pearl)}
\subtitle{(Representation Pearl)}
%\subtitle{(representation pearl)}


%\title{\textit{Representation Pearl:} Access Patterns: A Pure, Low-level Tensor Representation}
% \title{Access Patterns: A Pure, Low-level Tensor Representation}
% Access patterns: referentially transparent low-level tensor representation (for term rewriting?)
% emphasize that low-level is a hole, and we fill it
%\title{\textit{Representation Pearl:} Enabling Equality Saturation for Low-level Tensor Kernels with \g}
%\title{\textit{Representation Pearl}:
%Algebraic, Tensor-based Hardware--Software Programs in \g}
% \title{(todo: mention pearl in title) \g: A Representation for Tensor-Based Hardware--Software Programs}
%\date{December 2020}

\author{Gus Henry Smith}
\email{gussmith@cs.washington.edu}
\affiliation{%
  \institution{Paul G.~Allen School of Computer Science \& Engineering}
  \streetaddress{Some street address}
  \city{Seattle}
  \state{WA}
  \country{USA}
  \postcode{some post code}
}
\author{Andrew Liu}
\email{andy99@cs.washington.edu}
\affiliation{%
  \institution{Paul G.~Allen School of Computer Science \& Engineering}
  \streetaddress{Some street address}
  \city{Seattle}
  \state{WA}
  \country{USA}
  \postcode{some post code}
}
\author{sslyu}
\email{}
\affiliation{%
  \institution{Paul G.~Allen School of Computer Science \& Engineering}
  \streetaddress{Some street address}
  \city{Seattle}
  \state{WA}
  \country{USA}
  \postcode{some post code}
}
\author{Scott}
\email{}
\affiliation{%
  \institution{Paul G.~Allen School of Computer Science \& Engineering}
  \streetaddress{Some street address}
  \city{Seattle}
  \state{WA}
  \country{USA}
  \postcode{some post code}
}
\author{Joseph}
\email{}
\affiliation{%
  \institution{Paul G.~Allen School of Computer Science \& Engineering}
  \streetaddress{Some street address}
  \city{Seattle}
  \state{WA}
  \country{USA}
  \postcode{some post code}
}

\author{Zachary Tatlock}
\email{ztatlock@cs.washington.edu}
\affiliation{%
  \institution{Paul G.~Allen School of Computer Science \& Engineering}
  \streetaddress{Some street address}
  \city{Seattle}
  \state{WA}
  \country{USA}
  \postcode{some post code}
}

\author{Luis Ceze}
\email{luisceze@cs.washington.edu}
\affiliation{%
  \institution{Paul G.~Allen School of Computer Science \& Engineering}
  \streetaddress{Some street address}
  \city{Seattle}
  \state{WA}
  \country{USA}
  \postcode{some post code}
}

\author{mbt}
\email{}
\affiliation{%
  \institution{Paul G.~Allen School of Computer Science \& Engineering}
  \streetaddress{Some street address}
  \city{Seattle}
  \state{WA}
  \country{USA}
  \postcode{some post code}
}


\begin{abstract}

Tensor kernels in machine learning (ML)
  often correspond to pure mathematical expressions,
  making term rewriting an attractive strategy
  for optimization and mapping to specialized hardware accelerators.
%   ---%
%   especially given recent progress in equality saturation.
However,
  existing ML intermediate representations (IRs)
  tend to either be \textit{pure but high-level},
  making low-level rewrites
  to hardware targets inexpressible,
  or \textit{low-level but impure},
  hampering the use of term rewriting altogether.
%   leaving no suitable IR
%   for term rewriting
%   over low-level tensor kernels.

This paper introduces \g,
  a pure IR whose core abstraction---%
  the \textit{access pattern}---%
  enables
  low-level,
  layout-aware,
  hardware-centric
  program rewrites.
We demonstrate how term rewriting
  in \g
  can be used to 
  map program fragments
  to hardware accelerator invocations
  and
  automatically discover
  classic data layout transformations
  like \tcd{im2col}.
% Hm... not capturing this idea at the moment:
%  that facilitate hardware--software mapping
%  but previously required explicit manual implementation.
\g establishes a new foundation for
  exploring further term rewriting techniques
  in optimizing low-level tensor programs.
%  \hl{todo we don't ever explain hardware--software programs}



\end{abstract}

\begin{document}

\maketitle

\section{Introduction}
    
\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{access-pattern-examples-2x2.png}
    \caption{
      Four access patterns,
        representing different ways
        a kernel might access
        the same 3D tensor. 
      For example, (c) represents
        accessing a 3D tensor as
        a vector of 2D matrices.}
    \label{fig:access-pattern-examples}
    \vspace{-1em}
\end{figure}

% \hl{story: we want to even be able to discuss things like low-level data layout changes. then we set the goalposts: can do datalayout changes, can do eqsat. i.e.~don't make eqsat the motivation, make it the goalpost; make the motivation the simple task of even being able to represent these programs and transformations}

Machine learning (ML) and other
  high-performance computing (HPC)
  applications increasingly rely on
  specialized hardware accelerators to
  provide speed and energy efficiency~\cite{jouppi2017tpu, krizhevsky2012conv, reuther2019survey}.
This trend has highlighted the need
  for flexible accelerator support
  in domain-specific compilers like
  Halide~\cite{halide},
  TVM~\cite{tvm},
  TensorFlow/MLIR~\cite{tensorflow, mlir}, and
  PyTorch~\cite{pytorch}.

Adding accelerator support to
  an existing compiler typically
  uses custom pattern matching to
  map expensive tensor operations
  from applications down to
  accelerator invocations~\cite{
    yang2020interstellar, byoc}.
Pattern matching often additionally relies on
  various earlier transformations
  to canonicalize intermediate representations (IRs)
  %~\cite{??}
  and massage data layouts into
  formats matching accelerator requirements~\cite{nvidia2020nhwc}.
Even with these changes,
  users may need to manually modify their application to
  help the compiler discover opportunities
  for dispatching operations to accelerators, 
  such as by changing data types or unrolling loops.
    
In principle, term rewriting techniques~\cite{baader1998term}
  should be able to facilitate many of
  these transformation and mapping tasks
  within a compiler.
Halide and TVM already rely
  on extensive rewrite systems for
  optimizing scalar computations and
  simplifying loop bounds in order to
  support further downstream optimizations~\cite{newcomb2020halide-rewrite,
  hagedorn2020func-high-perf}.

Unfortunately, existing IRs in compilers for
  array/tensor programming DSLs tend to
  present abstraction and granularity mismatches
  that hamper term rewriting approaches.
Term rewriting is most easily applied in
  ``pure'' (side effect--free) IRs
  that support equational reasoning.
At the same time,
  mapping to accelerators requires considering
  low-level hardware details like data layout.
Existing pure IRs for ML frameworks are used
  primarily for high-level transformations
  (e.g., type elaboration and inlining)
  and do not expose low-level data layout details~\cite{
    relay}.
On the other hand,
  IRs used for crucial lower-level optimizations like
  operator fusion must support
  precise reasoning about memory use,
  and therefore are typically impure,
  hampering term rewriting.% approaches.

To help mitigate such impedance mismatches,
  we present \textit{\g}, a pure tensor program IR
  that enables hardware-level term rewriting.
\g is based on a simple
  \textit{access pattern} abstraction that
  supports expressing and reasoning about
  data layout transformations via
  syntactic rewrite rules.
When combined with standard arithmetic rewrites
  for per-tensor-element computations,
  access patterns enable implementing complex,
  transformations for accelerator support as
  compositions of simple rewrites.

Tensors are traditionally characterized
  by their \textit{shape},
  an $n$-tuple 
  %in $\mathbb{N}^n$
  of positive integers
  indicating the size of each
  of a tensor's dimensions.
  % , e.g., $(x, y, z)$ for a 3D tensor.
Access patterns instead characterize
  each tensor with two shapes, e.g.,
  \accesspatternshape{x}{y, z}, separating
  the dimensions which are \textit{iterated over} from
  the dimensions which are \textit{computed on.}
Figure~\ref{fig:access-pattern-examples}(c)
  depicts an example where a 3D tensor's
  first dimension is iterated over and
  some computation applied to each
  corresponding 2D matrix.

We demonstrate how \g
  enables implementing representative
  hardware-level transformation via term rewriting,
  including mapping computations
  to systolic arrays~\cite{jouppi2017tpu}
  (a common hardware module in ML accelerators)
  and automatically discovering the
  \tcd{im2col} data layout transformation~\cite{im2col},
  which enables mapping 2D convolutions
  to matrix multiplication hardware.
In particular,
  by employing the \textit{equality saturation}
  rewrite engine strategy~\cite{willsey2021egg},
  these transformations ``fall out for free''
  (i.e., without any carefully crafted
  rewrite orderings~\cite{phase-ordering}),
  from a handful of general rewrites concerning tensor
  transposition, Cartesian product, dot product, etc.,
  expressed in terms of access patterns.

To summarize, our contributions include:
\begin{itemize}
\item \textit{Access patterns},
  a tensor representation that employs a
  simple, extended tensor shape type to
  distinguish iteration and computation dimensions

\item The \g IR,
  a pure compiler IR that facilitates 
  term rewriting to enable support for
  specialized accelerators
  
\item A library of generic rewrites over \g programs
  
\item Case studies demonstrating how
  \g enables automatically discovering
  key transformations for mapping
  applications to custom accelerators
  via equality saturation with the
  \tcd{egg}~\cite{willsey2021egg} library.
\end{itemize}

The rest of the paper is organized as follows:
Section~\ref{sec:background} provides background
  and briefly surveys closely related work.
Section~\ref{sec:matmul} motivates
  \g via a running example exploring
  pure matrix multiplication.
Section~\ref{sec:glenside} details the
  design and implementation of \g.
Section~\ref{sec:case-studies} details
  case studies showing the potential
  benefits of \g's term rewriting
  approach to low-level tensor program
  transformations.
%Section~\ref{sec:conclusion}
%  discusses future work and concludes.

  
  

% With the rising interest in
%   machine learning accelerators,
%   mapping pieces
%   of a machine learning workload
%   to custom hardware
%   has become a primary concern
%   for modern machine learning compilers
%   \cite{chen2018tvm, tensorflow, pytorch, etc}.
% %A task of concern
% %  for machine learning accelerator
% %  designers
% %  and compiler engineers
% %  is
% %  that of
% %  mapping
% %  pieces of 
% %  a machine learning workload
% %  to custom hardware.
% %This interest was
% %  recently signaled
% %  by 
% %  the machine learning compiler TVM
% %  with 
% %  the merging of the 
% %  Bring Your Own Codegen
% %  (BYOC)
% %  framework,
% %  which allows for the matching
% %  and offloading
% %  of high-level program patterns.
% %The BYOC framework
% %  gives developers a way
% %  to match on simple high-level
% %  program patterns,
% %  allowing them to then generate 
% %  their own device-specific code
% %  for the matched program snippets,
% %  while allowing the rest of the program
% %  to be compiled
% %  via TVM's standard
% %  compilation flow.
% %To further signal
% %  the desire for hardware matching,
% %  we need only to point out that
% %  BYOC is not the only such framework
% %  in TVM
% %  for matching on program patterns:
% %  TVM also supports
% %  the matching and replacing of 
% %  lower-level program patterns
% %  through a process it calls
% %  \textit{tensorization}.
% Many of these compilers
%   are powered by
%   term rewriting systems,
%   which implement potent optimizations
%   by matching and rewriting program patterns
%   over the compiler's
%   IR.
% However,
%   these IRs are 
%   too high-level
%   for the purpose of mapping to accelerators,
%   as they abstract away key details
%   such as data layout.
% While these compilers
%   will also have
%   lower-level IRs
%   for hardware-specific optimizations,
%   they are generally impure,
%   and so are incompatible
%   with term rewriting systems.
% Thus, we are faced with
%   an impedance mismatch:
%   we require an IR that is pure---%
%   as many high-level machine learning IRs are---%
%   but still capable
%   of representing
%   hardware-specific details---%
%   as their low-level, impure counterparts can.


% ML compilers use rewriting a lot
% But they use traditional term rewriting
% Very order sensitive
% MEANS THAT DEVELOPERS HAVE TO CHANGE MODEL TO USE ACCELERATOR
%   -- because you have to set things up so rewrites will map to accelerator

% Would love to use eqsat which does all orders of all rewrites all the time
% Does great in a bunch of other domains already
% Get stuck when we try to apply to ML

% PROBLEM : IRs all at the "wrong level"
% Relay : too "high level" -- need to see inside tensor ops
% TensorIR : too "low level" -- imperative (rewrites destructive), impedence

% Key Insight : access patterns
% Need to separate access from computation
% Helps with mapping to accelerator

% Then design IR around that : Glenside
% add compute operators

% Then add rewrites
% discover stuff like im2col

% contributions / rest of paper organized

% 1. access patterns -- enable data layout: key for accel mapping
% 2. IR design -- glenside in "goldilocks zone"
% 3. key rewrites
% 4. case studies demonstrating Glenside + EqSat gives stuff like im2col
 
%Yet for 
%  exploratory applications,
%  such as the user-defined
%  pattern matching
%  of BYOC and tensorization,
%  they are potentially very brittle.
%Even if these tools
%  apply rewrites
%  to experimentally transform the program
%  with the goal of uncovering
%  places where the 
%  user's patterns can be matched,
%  the order in which rewrites are applied
%  can greatly affect
%  the term rewriter's success
%  at finding matches.
%To deal with this brittleness,
%  users of these frameworks
%  may be forced to
%  modify their existing patterns
%  or add completely new patterns
%  for each new network
%  they want to handle,
%  requiring time
%  and domain knowledge
%  they may not have.
%\textit{Equality saturation}
%  is a method
%  explicitly developed
%  to avoid this phase-ordering problem
%  \cite{egg, others}.
%%Key to equality saturation are
%%  equality graphs
%%  (\egrs),
%%  a data structure
%%  which non-destructively
%%  applies program rewrites,
%%  storing all
%%  previously-discovered
%%  programs.
%The recent works
%  Tensat \cite{yang2021equality}
%  and SPORES \cite{wang2020spores}
%  have even demonstrated
%  the potential of
%  equality saturation
%  in the realm of optimizing tensor programs.
%But for our specific application---%
%  matching program patterns 
%  to offload to custom hardware---%
%  we are faced with an impedance mismatch.
%Existing tensor IRs
%  are either too high-level,
%  as is the case with TVM's
%  IR Relay \cite{relay},
%  or low-level but
%  incompatible with equality saturation,
%  as is the case with TVM's
%  IR TIR.
%Even the IRs 
%  of Tensat and SPORES,
%  which were specifically designed for
%  equality saturation,
%  are too high-level
%  for our purposes.
% In this work,
%   we present
%   \g{}:
%   a pure IR
%   for
%   exploring
%   hardware-level
%   transformations
%   of tensor programs.
% The key insight
%   behind \g
%   is
%   its tensor representation,
%   the \textit{access pattern.}
% While tensors are traditionally defined
%   by a single shape,
%   e.g.~$(x, y, z)$
%   for a three-dimensional tensor,
%   access patterns
%   split this into two shapes,
%   e.g.~\accesspatternshape{x}{y, z},
%   separating the dimensions which are 
%   \textit{iterated over}
%   from the dimensions which are
%   \textit{computed on.}
% In this example,
%   depicted in
%   Figure \ref{fig:access-pattern-examples}(c),
%   we are treating the first dimension
%   as our iteration dimension,
%   viewing the three-dimensional tensor
%   as a vector of two-dimensional matrices.
% With access patterns
%   as our foundation,
%   the design of \g
%   is mostly comprised of
%   redefining common 
%   tensor operations---%
%   e.g.~transpose and dot product---%
%   with access pattern semantics.
% This sentence seems out of place now, but i still like it:
%This enables
%  rank-polymorphic
%  redefinitions
%  of common tensor operations,
%  which in turn allow \g
%  to express
%  many common 
%  machine learning tensor kernels
%  concisely and intuitively.

% Using \g,
%   we demonstrate
%   how we can use term rewriting
%   to perform a number of hardware-level
%   transformations,
%   including mapping computations
%   to a systolic array
%   (a common hardware module
%     in ML accelerators)
%   and  discovering the
%   \tcd{im2col} data layout transformation,
%   which enables us to perform
%   2D convolutions
%   on matrix multiplication hardware.
% These examples are implemented
%   with a handful
%   of general rewrites
%   defined over \g.

% \g
%   also
%   enables us
%   to take advantage
%   of recent advances in term rewriting systems
%   when designing hardware-level
%   transformations.
% Traditional
%   term rewriting systems
%   are vulnerable to
%   the \textit{phase ordering problem,}
%   in which the order of rewrites
%   may determine whether
%   a certain pattern is found.
% This is especially pernicious
%   when mapping programs to hardware,
%   as it can determine the success
%   of the exploratory rewrites
%   the compiler uses
%   to expose the hardware mapping.
% \textit{Equality saturation}
%   is a method
%   explicitly developed
%   to avoid this phase-ordering problem
%   \cite{others}.
% We use the egg
%   equality saturation library \cite{willsey2021egg}
%   to implement our term rewriting system.

% To summarize,
%   the contributions of this paper
%   are
  
% \begin{itemize}
% \item
% The access pattern,
%   a new tensor representation
%   which disambiguates iteration
%   and computation,

% \item
% The \g language,
%   a collection of operators
%   over access patterns,
  
% \item
% A set of rewrites
%   over \g, and finally,
  
% \item
% Case studies
%   of the transformations
%   which these rewrites enable,
%   implemented with the \tcd{egg}
%   equality saturation library.
% \end{itemize}

% The rest of this paper
%   is organized as follows:
%   \hl{blah blah}.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{matmul-conv2d-access-patterns.png}
    \caption{
        Access pattern visualizations for
        matrix multiplication (left) and
        2D convolution (right) in \g.
        Similar structure enables
        reusing optimizations and applying
        the same hardware mapping strategies
        across different application/accelerator
        targets.}
    \label{fig:matmul-conv2d-access-patterns}
    \vspace{-1em}
\end{figure}

%    
    % A visual representation
    %   of how we implement
    %   matrix multiplication of $A$ with $B$ (left)
    %   and 2D convolution 
    %   of an image with a set of filters (right)
    %   using access patterns
    %   in \g.
    % Access patterns
    %   enable simple,
    %   rank-polymorphic
    %   definitions of 
    %   operators like
    %   Cartesian product
    %   and
    %   dot product.
    % With these operators,
    %   it turns out
    %   many kernels
    %   (such as matrix multiplication
    %     and 2D convolution)
    %   have remarkably similar structure,
    %   which \g can exploit
    %   when mapping programs
    %   to hardware.


% \begin{figure*}
% \begin{subfigure}[t]{0.49\textwidth}
%     \begin{lstlisting}
% dotProd  ;  ([f64], [f64]) -> f64
% cartProd ;  [f64] -> [f64] -> [(f64, f64)]
% rows     ;  [[f64]] -> [[f64]]
% cols     ;  [[f64]] -> [[f64]]

% matMul ;  [[f64]] -> [[f64]] -> [f64]
% (matMul a b) :=
%   (map dotProd (cartProd (rows a) (cols b)))
% \end{lstlisting}
% \end{subfigure}
% \begin{subfigure}[t]{0.49\textwidth}
%     \centering
% \begin{lstlisting}
% cartProd   ;  [f64] -> [f64] -> [[([f64], [f64])]]
% tensorMap2 ;  (f64 -> f64) -> [[f64]] -> [[f64]]

% matMul ;  [[f64]] -> [[f64]] -> [[f64]]
% (matMul a b) :=
%   (tensorMap2 dotProd (cartProd (rows a) (cols b)))
% \end{lstlisting}
% \end{subfigure}
% \caption{Our first (left) and second (right) attempts at implementing a pure matrix multiplication.}
%     \label{fig:matmul-haskell}
% \end{figure*}

\section{Background and Related Work}
\label{sec:background}

\g is designed to help target
  tensor hardware accelerators and
  builds on past work in
  tensor IRs and term rewriting.

% Here,
%   we give some background
%   on equality saturation,
%   tensor IRs, 
%   and the process
%   of mapping these programs
%   to hardware.


\subsection{Machine Learning Accelerators}

A variety of accelerators~\cite{
    jouppi2017tpu, chen2016eyeriss, moreau2018vta, markidis2018tensorcore, nvdla}
  have been developed 
  to provide efficient implementations
  of tensor operators for ML applications.
These devices accelerate tensor operators 
  through hardware parallelism, 
  simultaneously applying related operations
  across many tensors in the accelerator's memory,
  often laid out according to custom rules
  that facilitate hardware optimization.
Tensor program compilers must translate
  expensive application code fragments
  down to accelerator invocations that
  adhere to these layout rules,
  which often involves both
  (a) higher-level transformations like
  tensor reshaping to match accelerator size bounds and
  loop unrolling to expose optimization opportunities, and
  (b) lower-level transformations like
  operator fusion and \tcd{im2col}
  to match accelerator calling conventions and
  even implement different operations
  using the same accelerator,
  e.g., on systolic arrays~\cite{im2col, jia2014semantic}.
  
% Hence, it becomes the task of the compiler 
%   to ensure that data 
%   can be efficiently offloaded 
%   to these accelerators 
%   in the expected format,
%   sometimes requiring higher-level program transformations
%   to avoid redundant transformations.
% \hl{Maybe this isn't the place for im2col?} Additionally, 
%   certain layout transformations 
%   can be used to implement different operations using the same accelerator,
%   as in the above-mentioned \tcd{im2col} example.

\subsection{Tensor IRs and Compilers}

%Machine learning workloads
%  are generally viewed
%  as a sequence of 
%  of \textit{tensor kernel} invocations,
%  where tensor kernels
%  are large operations
%  over multidimensional arrays (tensors)
%  such as 2D convolution
%  or dense matrix multiplication.
%Machine learning frameworks
%  (such as TensorFlow \cite{tensorflow}
%    and PyTorch \cite{pytorch})
%  and machine learning compilers
%  (such as TVM \cite{chen2018tvm})
%  can optimize
%  machine learning workloads
%  at a number of levels,
%  which often result
%  in each framework
%  having a number of different IRs.
%TVM, for example,
%  can optimize machine learning workloads
%  at a high-level
%  \hl{example?}
%  using its high-level representation
%  Relay \cite{relay},
%  but
%  low-level optimizations
%  such as loop blocking and reordering
%  must be done 
%  in its lower-level IRs,
%  TIR and TE.

% \hl{not mentioning hand-optimized kernels like CuDNN,
% but maybe that's OK for focus / space}

Tensor compilers for ML and HPC applications strive
  to balance clear, high-level operator semantics
  and support for the low-level optimizations
  necessary to target specialized accelerators.
Halide~\cite{ragan2013halide}
  achieves this balance by separating
  operator \textit{specifications} (what is computed) from
  \textit{schedules} (how, when, and where
  each output element is generated).
This style of separation has proven
  highly effective across both
  application domains and hardware targets;
  numerous compilers including TVM~\cite{chen2018tvm},
  FireIron~\cite{hagedorn2020fireiron},
  LIFT~\cite{lift}, and Accelerate~\cite{accelerate}
  follow variations on this strategy.
  
The specification/schedule separation approach
  allows the same high-level program (specification)
  to be flexibly optimized for and mapped to
  different hardware targets by applying different schedules.
From this perspective,
  schedules represent different rewriting strategies
  to explore various loop ordering and memory layouts;
  in LIFT and Accelerate these
  take the form of functional combinators
  closely related to \g's approach.
As in classic term rewriting,
  experts must often carefully craft
  schedules for each target to achieve
  the best performance and mitigate
  phase ordering challenges~\cite{phase-ordering},
  though recent projects have produced promising results
  in automatic scheduling~\cite{
    chen2018autotvm, zheng2020ansor, anderson2020learning}.

Other tensor IRs like
  TACO~\cite{taco}, Keops~\cite{keops},
  and COMET~\cite{tian2021highperformance}
  rely on \textit{index notation}\footnote{
    Index notation is closely related to
    ``Einstein notation'' where reduction
    indices are implicit.}
  to concisely express tensor operators
  and simplify optimization by
  uniformly representing
  per-output-element computations.
These approaches also rely on
  rewriting passes to generate
  kernel implementations specialized to
  tensor sparsity / density,
  operator combinations arising in
  the source application, and
  details of the target hardware.
In Section~\ref{sec:matmul} we discuss
  some of the trade-offs of these approaches
  with respect to other rewriting strategies.
 
Finally, polyhedral compilers~\cite{polyhedral-survey}
  like Tensor Comprehensions~\cite{vasilache2018tensor}
  and Tiramisu~\cite{tiramisu}
  optimize loop-filled programs
  by modeling loop nests as polyhedra
  and applying geometric transforms.
The polyhedral approach exploits
  regular loop structure,
  but is also restricted
  to geometrically affine transformations.
In contrast, term rewriting is
  neither guided nor restricted by
  geometric constraints, making
  these approaches broadly complementary.


% Accelerators and parallel architectures 
%   present many opportunities 
%   for tuning and improving tensor operator implementations for the best performance.
% Many systems, 
%   including deep learning frameworks like PyTorch~\cite{pytorch},
%   make extensive use of hand-optimized tensor operator implementations
%   (such as those in the CuDNN library~\cite{chetlur2014cudnn}).
% While these hand-optimized implementations
%   achieve strong performance,
%   various systems, including \g,
%   have been proposed to automatically
%   generate optimized tensor operators
%   to reduce development time,
%   support varied hardware platforms,
%   and avoid reliance on a small number of experts.
% These tensor compilers 
%   rely on specifying tensor computations 
%   in a form that allows for
%   conveniently exploring options for optimizations, 
%   such as by changing loop orderings
%   to best utilize caches.

% \hl{Surely there's a better summary/gloss here.}
% Many tensor operator compilers 
%   use index notation
%   to specify tensor operators
%   (defining how each element of the output
%   is to be computed
%   based on corresponding elements 
%   of the input).
% TACO~\cite{taco} uses index notation 
%   for automatically generating and optimzing both sparse and dense tensor computations,
%   permitting particular dimensions 
%   to be specified as dense or sparse,
%   introducing various structures for reasoning about how 
%   to iterate over sparse dimensions.
% %Tensor operators in TACO
%   %are described using index notation, 
%   %with particular tensor dimensions 
%   %being either dense or sparse.
% %\cite{taco} initially 
%   %focused on compiling for CPU, 
%   %while subsequent work~\cite{senanayake2020}
%   %has extended TACO to target
%   %GPUs and vector units.
% %  is a compiler notable for its handling
% %  of computations over sparse tensors.
% %\hl{unsure how to draw any connection here, but i feel like i need to at least mention taco}
% Keops~\cite{keops} also uses
%   index notation to describe tensor operations
%   for both dense and spare matrices,
%   but introduces another matrix representation
%   called symbolic matrices, 
%   in which each element of the symbolic matrix
%   is computed from two smaller data matrices.
% Computations on symbolic matrices 
%   can more easily be deployed to GPUs and other accelerators 
%   than sparse computations
%   but require less data movement than dense computations.
% %% I think MLIR isn't relevant in this discussion: IRs built _in_ MLIR might be
% %MLIR~\cite{mlir} is a general-purpose compiler framework intended to facilitate the development of IRs.
% %While MLIR itself does not have a built-in notion of tensorization, it has been used to implement polyhedral IRs and other tensor operator representations.
% COMET~\cite{tian2021highperformance} is another
%   CPU-focused sparse tensor algebra IR,
%   implemented using MLIR~\cite{mlir},
%   that additionally uses information about sparse storage formats 
%   in its representation 
%   to increase temporal and spatial locality.

% Halide~\cite{ragan2013halide}
%   %while not a machine learning compiler
%   has been influential
%   in using a representation
%   that separates
%   an tensor operator's specification (what it computes)
%   from its schedule (what order elements are computed),
%   allowing many optimizations
%   to be described as different schedules
%   for the same specification
%   and facilitating operator fusion.
% As discussed in~\cite{newcomb2020halide-rewrite},
%   Halide applies a term rewriting system
%   for producing optimized code.
%   \hl{Any insight for why Halide is amenable to a rewrite system?}
% TVM~\cite{chen2018tvm}
%   uses a similar specification-schedule split
%   for describing its tensor operators.
% FireIron~\cite{hagedorn2020fireiron} is another system
%   using a split between a computation specification 
%   and its realization
%   (using a decompositions, rather than Halide-style schedules,
%   for scheduling parts in a top-down style).
% \g can be understood
%   as a further refinement
%   on an algorithm's specification,
%   disambiguating
%   how data is accessed
%   from the computation being performed
%   over the data,
%   enabling transformations
%   like mapping to hardware
%   before the program is scheduled.
%   \hl{What does it mean
%   that mapping to hardware
%   is done before the program is scheduled?}

% Like \g, several other systems 
%   use functional representations
%   of tensor operators.
% LIFT~\cite{lift}
%   separates specifications from schedules
%   using combinators like maps and reductions to specify schedules,
%   an idea further explored in~\cite{hagedorn2020func-high-perf}.
% \hl{anything else to say about LIFT? Contrast with Halide/TVM's scheduling languages?}
% Accelerate~\cite{accelerate} 
%   uses functional combinators 
%   to map Haskell code 
%   to GPU programming primitives.
% \hl{Discussion comparing these to \g?}

% The polyhedral representation~\cite{polyhedral-survey} 
%   has also proven useful
%   for expressing tensor operators.
% %Polyhedral compilation~\cite{polyhedral-survey}
%   %is a powerful alternative
%   %to term rewriting-based compilers.
% Polyhedral compilers optimize
%   loop-filled programs
%   by modeling loop nests
%   as polyhedra
%   and transforming them geometrically.
% Tensor Comprehensions~\cite{vasilache2018tensor}
%   and Tiramisu~\cite{tiramisu}
%   are examples of machine learning compilers
%   powered by the polyhedral framework.
% \hl{Need note about deploying to accelerators?}
% \hl{Implications for term rewriting?}
% Multidimensional homomorphisms~\cite{multidimensional-homomorphism} 
%   are another mathematical representation 
%   of tensor operations
%   that has been amenable to automatic parallelization 
%   and allows for hierarchical composition.
%   \hl{any comment in relation to \g?}

%There are at least as many
  %tensor IRs
  %as there are
  %deep learning frameworks,
  %and many frameworks
  %use more than one.
%TVM \cite{chen2018tvm},
  %for example,
  %uses the high-level
  %Relay IR~\cite{relay}
  %for optimizations
  %in the sequencing of tensor kernels,
  %while it uses low-level TIR
  %to optimize kernels themselves.
%\hl{If you're going to complain about TIR not being pure, you need to establish that purity is a requirement before this. Maybe discuss this later}
%Relay and TIR
  %are a perfect example
  %of our impedance mismatch problem:
  %Relay is pure but too high-level,
  %while
  %TIR is low-level but impure.


% Should we talk about BYOC here?
%Recently,
%  the Bring Your Own Codegen
%  (BYOC)
%  framework
%  was merged into TVM \cite{byoc}.
%BYOC
%  is novel,
%  as it allows for the 
%  fast implementation
%  of a full compiler
%  for deep learning models
%  down to custom hardware backends
%  with fairly little additional code,
%  assuming the users
%  have written a codegen
%  for their hardware
%  already.
%BYOC includes
%  a matching framework,
%  for matching
%  user-specified
%  patterns
%  of Relay code.
%As we will see
%  in Section \ref{sec:case-study-tensorization},
%  one of our primary case studies
%  of \g{}
%  is as a pattern matcher
%  for matching such computational patterns---%
%  in fact,
%  it was the original
%  intended use case
%  of \g{}!
%In some sense,
%  the two pattern matchers
%  are incomparable,
%  because,
%  as we've described above,
%  \g{}
%  and Relay
%  operate at different levels
%  of abstraction.
%Yet if we ignore the differences
%  in the levels of abstraction,
%  we can compare
%  the matchers
%  in how they operate.
%In this comparison,
%  \g{} shines through
%  with the help of
%  the \egr{}.
%Because rewrites
%  are so cheap and easy
%  within the \egr{},
%  we can run a huge number
%  of exploratory rewrites,
%  potentially finding places
%  to map in user-defined
%  patterns.
%BYOC,
%  on the other hand,
%  expects the user to provide
%  multiple equivalent patterns
%  to cover the numerous ways
%  their pattern might appear 
%  in the program
%  \hl{(verify this)}.

%Named tensors \cite{chiang2021named}
%  are a primary inspiration
%  for the access pattern data structure
%  at the core of \g.
%%Tensors have a specific layout
%%  in memory,
%%  a hardware detail
%%  that often bubbles up
%%  even to the highest levels
%%  of tensor abstractions.
%%For example,
%%  in both TVM and TensorFlow,
%%  the \tcd{conv2d} operator
%%  requires the user to specify
%%  the layout of the activation tensor---%
%%  \tcd{NCHW} or \tcd{NHWC},
%%  where \tcd{N} is the batch dimension,
%%  \tcd{C} is the channels dimension,
%%  and \tcd{H} and \tcd{W} are the spatial
%%  (height and width)
%%  dimensions.
%%Rather than pass layout information
%%  externally,
%%  named tensors
%%  incorporate dimension names
%%  directly into the tensor abstraction.
%%Named tensors
%%  acknowledge the fact
%%  that
%%  not all dimensions
%%  are made the same,
%%  and that different dimensions
%%  have different functions.
%Named tensors
%  assign names
%  to the dimensions of a tensor,
%  increasing code readability
%  and acknowledging that different dimensions
%  have different functions.
%Though
%  we do not actually adopt
%  this notation
%  in \g{}---%
%  the dimensions of tensors
%  are still just indexed
%  with integers---%
%  this core idea
%  is reflected
%  in the design
%  of access patterns,
%  which encode the fact that
%  some dimensions are
%  for
%  \textit{iterating over,}
%  while others are meant to be 
%  \textit{computed upon.}

%Many machine learning frameworks
  %have an optimization step
  %in which they match program patterns
  %to efficient low-level implementations,
  %whether that be offloading to hardware
  %or calling into a tuned library.
%TVM \cite{chen2018tvm}, 
%% Steve: I would say Latte is not very relevant here
%% It provides AD, but it requires the user to manually implement just about everything else
  %Latte \cite{latte},


%% Steven: Not clear it's relevant to this context
%Norman Ramsey instruction selection~\cite{dias2010instruction-selection}

%\hl{even further back:} apl?~\cite{apl-survey} % probably no

%\subsection{Term Rewriting, Equality Graphs, and Equality Saturation}
\subsection{Term Rewriting and Equality Saturation}

Term rewriting is a classic
  program optimization technique~\cite{baader1998term}
  that relies on iteratively applying
  rewrite rules of the form $\ell \xrightarrow{} r$:
  when part of a program
  matches the pattern $\ell$
  under substitution $\sigma$,
  it is rewritten into $\sigma(r)$.
This approach is ubiquitous,
  appearing in a large proportion
  of both mainstream and DSL compiler
  to implement preprocessing,
  strength reductions,
  peephole optimizations, etc.~\cite{garavel2018rewrite-context}
  
Classic term rewriting systems where
  rewrites are applied destructively suffer
  phase ordering problems~\cite{phase-ordering}:
  the order in which rewrites are applied can
  enhance or severely diminish performance.
Recent work has shown how program synthesis
  can help address this challenge in
  peephole optimizers like Halide's
  scalar expression rewriter~\cite{
    newcomb2020halide-rewrite}.
  
Advances in alternate rewriting techniques
  like equality saturation~\cite{tate2009equality}
  also mitigate phase ordering by exploiting
  the e-graph data structure from SMT solvers
  to repeatedly apply all rewrites simultaneously,
  thus obviating rule ordering considerations.
In particular,
  the \tcd{egg} library~\cite{willsey2021egg}
  has been used to develop new synthesis and
  optimization tools across diverse domains~\cite{
    herbie, szalinski, wang2020spores},
  including DSP compiler vectorization~\cite{
    vanhattum2021vectorization} and
  tensor computation graphs~\cite{yang2021equality}.

\g provides the first tensor IR
  amenable to equality saturation by
  introducing access patterns to
  provide pure, higher order tensor
  kernel combinators that support
  rank-polymorphism without the need
  for binding structures like
  anonymous functions or index notation.

% {\color{red} \noindent\rule{8.5cm}{10pt}}

% \g utilizes
%   the power of equality saturation
%   in
%   low-level tensor program
%   transformation---%
%   namely, mapping programs
%   to custom hardware.
  
% Term rewriting
%   is a classic technique
%   in program optimization
%   in which rewrite rules of the form
%   $\ell \xrightarrow{} r$
%   are applied over a program.
% When an expression in the program
%   matches
%   the pattern $\ell$,
%   it is rewritten
%   into the pattern $r$.
% In classic term rewriting systems,
%   the ordering
%   in which rewrites are applied
%   can affect whether optimizations
%   are found,
%   as some rewrites
%   may reverse or muddle
%   the effects
%   of other rewrites.
% Equality saturation
%   is a technique
%   developed to sidestep
%   this 
%   phase ordering problem.
% The technique
%   has recently re-emerged
%   with the development
%   of the \tcd{egg} library
%   \cite{willsey2021egg},
%   and with the successful application
%   of equality saturation
%   to floating point optimization \cite{herbie}
%   and CAD program simplification \cite{szalinski},
%   among other varied fields.

% %A primary feature
% %  of equality saturation
% %  is its ability to
% %  efficiently store
% %  all previously discovered
% %  versions 
% %  of the program
% %  being transformed.
% %The data structure which enables this
% %  is the
% %  \textit{equality graph}
% %  (\egr),
% %  which resembles
% %  a standard program graph (\hl{not sure on words to use here?}),
% %  except
% %  each node in the program
% %  is replaced with a \textit{set} of nodes,
% %  called an \textit{equivalence class}
% %  or \textit{eclass.}
% %All nodes within an eclass
% %  are equivalent, and thus 
% %  can be substituted for one another
% %  to produce an equivalent program.
% %This property is only possible
% %  when the language used
% %  within the \egr
% %  is pure,
% %  which thus becomes a requirement
% %  for any language
% %  we'd like to transform
% %  with equality saturation.
  
% Diospyros \cite{vanhattum2021vectorization}
%   is a vectorizing compiler
%   for DSPs
%   which uses equality saturation
%   to match program fragments
%   to hardware primitives.
% Diospyros
%   starts from scalar programs
%   and, via symbolic evaluation
%   and equality saturation,
%   lifts them to vectorized
%   expressions,
%   while \g starts
%   from a higher level of abstraction
%   and searches downwards.
  
% SPORES \cite{wang2020spores}
%   and Tensat \cite{yang2021equality}
%   utilize equality saturation
%   to optimize
%   linear algebra expressions
%   and tensor computation graphs,
%   respectively.
% While both
%   show the potential
%   of equality saturation
%   for tensor program transformation,
%   both of their IRs
%   are too high-level
%   for the task of mapping to hardware.

% TODO mention somewhere in here that lambdas (which a lot of langs will use) are a problem for first-order rewrite systems


%\section{Motivating Example: A Pure Matrix Multiplication}
%\section{Motivation: Pure Matrix Multiplication}
%\section{Inspiration from Pure MatMul}
\section{From Pure \texttt{matMul} to IR Design Goals}
\label{sec:matmul}

% Many ML and HPC workloads are dominated by
%   evaluating compositions of tensor algebra operators
%   which, at a high level, correspond to
%   pure mathematical expressions.
% This make functional programming techniques attractive,
%   but requires careful design to ensure that
%   nested operators remain compositional with
%   respect to their output shapes.
% To highlight these design constraints and
%   motivate access patterns,
%   in this section we walk through a simple
%   matrix multiplication example to
%   illustrate the pitfalls that access patterns in \g
%   help mitigate.
  
Applying functional programming techniques
  and term rewriting to tensor IRs
  requires careful design.
For example,
  we must ensure operators are compositional
  with respect to tensor shapes,
  and that the representation supports
  generic rules within the
  target rewrite engine.
To highlight such constraints and
  motivate access patterns in \g,
  this section illustrates potential pitfalls
  with a simple matrix multiplication example.

\subsection{Pure Matrix Multiplication}
\label{subsec:pure-matmul}

We write
  \tcd{f64} for the type of 64-bit floats and
  \tcd{[A]} for vectors over type \tcd{A}.
Using this notation, we can specify operators like
  dot product and 2D matrix transpose as:
\begin{align*}
    \mcd{dotProd} &
    \mcd{ : [f64] * [f64] -> f64} \\
    \mcd{trans2} &
    \mcd{ : [[f64]] -> [[f64]]}
\end{align*} 

% \begin{itemize}[leftmargin=*]
% \item
%   \tcd{[[f64]]} as the type of 2D matrices

% \item
%   \tcd{trans2 : [[f64]] -> [[f64]]} as matrix transpose

% \item 
%   \tcd{dotProd : [f64] * [f64] -> [f64]} 
%   as dot product
% \end{itemize}

%   so that, e.g.,
%   \tcd{[f64] * [f64] -> [f64]} represents
%   the type of a dot product operator \tcd{dotProd},
%   \tcd{[[f64]]} represents the type of
%   2D matrices of 64-bit floats, and
%   \tcd{[f64] * [f64] -> [f64]} represents
%   the type of a dot product operator.

% \noindent
% Assuming row-major layout,
%   implementing 2D matrix multiplication
%   on inputs \tcd{P} and \tcd{Q},
%   requires computing an output matrix
%   \tcd{R} such that:
% $$
%   \mcd{R[i][j] = dotProd(P[i],\, trans2(Q)[j])}
% $$

\noindent
Implementing 2D matrix multiplication
  on inputs $P$ and $Q$ requires computing
  an output matrix $R$ where
  $R_{ij} = \Sigma_k P_{ik} \, Q_{kj}
          =  P_i \cdot Q^{T}_{j}$. %\mcd{dotProd}(P_i, Q^{T}_{j}).$ 
The need to compute \tcd{dotProd} for every pair
  of a row from $P$ and a column from $Q$
  suggests map and Cartesian product operators
  which we might specify with:
\begin{align*}
    \mcd{map} &
    \mcd{ : (A -> B) * [A] -> [B]} \\
    \mcd{cartProd} &
    \mcd{ : [A] * [B] -> [A * B]}
\end{align*}
Naively, we can almost implement matrix multiplication as:
{\color{red} \begin{align*}
  & \mcd{matMul(P, Q) :=} \\
  & \;\;\;\;\; \mcd{map(dotProd, cartProd(P, trans2(Q)))}
\end{align*} }
% {\color{red} $$
%   \mcd{map(dotProd, cartProd(P, trans2(Q)))}
% $$ }
However, the result type will have been
  flattened to just {\color{red}\tcd{[f64]}},
  making it impossible to compose with other matrix
  operators that expect \tcd{[[f64]]} inputs.

Our first problem is that
  the \tcd{cartProd} specification above
  ``forgets'' the shape of its arguments.
We could change this specification to
  arrange the output as a matrix:
  % according to  the positions of its argument's indices
$$
  \mcd{cartProd2D : [A] * [B] -> [[A * B]]}
$$
But this result type prevents
  directly mapping \tcd{dotProd}.\footnote{
    This simple type does not specify how
    \tcd{cartProd2D} orders its output
    relative to its input vectors.
    We assume the order
    expected for matrix multiplication.}
Now the problem is that \tcd{map}
  only applies a computation by iterating
  over the first (outermost) dimension of a tensor.
If we specialize \tcd{map} to iterate
  over the second dimension:
$$
  \mcd{mapAt2 : (A -> B) * [[A]] -> [[B]]}
$$
then we can implement a compositional
  \tcd{matMul} operator that correctly produces
  results of type \tcd{[[f64]]} as:
\begin{align*}
  & \mcd{matMul(P, Q) :=} \\
  & \;\;\;\;\; \mcd{mapAt2(dotProd, cartProd2D(P, trans2(Q)))}
\end{align*}

\subsection{\g Design Constraints and Goals}

This style of pure, higher-order functional
  program representation enables
  term rewriting and equational reasoning
  via rules like:
\begin{align*}
  \mcd{dotProd(P, Q)}
    & \leftrightsquigarrow
      \mcd{dotProd(Q, P)} \\[2pt]
  \mcd{trans2(trans2(P))}
    & \leftrightsquigarrow
      P \\[2pt]
  \mcd{map(f, map(g, P))}
    & \leftrightsquigarrow
      \mcd{map(f$\,\circ\,$g, P)} \\[2pt]
  \mcd{mapAt2(f, trans2(P))}
    & \leftrightsquigarrow
      \mcd{trans2(mapAt2(f, P))} % \\[2pt]
\end{align*}

% \footnote{
%     In these rules, we assume \tcd{reduce}
%     (``\tcd{fold}'') requires its first argument
%     to be an associative and commutative operator.}
%   \mcd{reduce(f, trans2(P))}
%     & \leftrightsquigarrow
%       \mcd{reduce(f, P)} \\[2pt]
%   \mcd{reduce(f, cartProd(P, Q))}
%     & \leftrightsquigarrow \\
%       \omit\rlap{\tcd{\hspace{0.5in}
%          reduce(f$\,\circ\,$swap, cartProd(Q, P))}}

However, some of these rules depend on the
  shapes of dimension-specific operators aligning.
What happens when we need to support
  higher-dimensional tensors?
Without a mechanism to abstract
  which dimensions of a tensor
  are being iterated vs. computed over,
  we would have to generate versions of
  each rule for every combination of dimensions.
Worse, these problems
  do not only affect rewrite rules;
  they also lead to code blowup just to
  specify all the variants of tensor kernels
  that arise in practice.

One strategy to address these challenges is
  adding support for anonymous functions (``lambdas''),
  currying, and closures to the 
  tensor program representation.
These features can provide sufficient
  flexibility to handle shape alignment
  issues that otherwise may require
  dimension-specific operators like
  \tcd{cartProd2D} and \tcd{mapAt2} above.
For example, given curried versions
  of \tcd{dotProd} and \tcd{map},
  we could have used such features
  to implement a curried \tcd{matMul} as:
\begin{align*}
  & \mcd{matMul' P Q :=} \\
  & \;\; \mcd{ \
      map' ($\boldsymbol\lambda\,$r =>\
        map' (dotProd' r) (trans2 Q)) P}
\end{align*}
Alternatively, some IRs rely on index notation
  for even pithier implementations like:
$$
  \mcd{matMul(P,Q)[i,j] := dotProd(P[i], trans2(Q)[j])}
$$

Unfortunately, these approaches all rely on some
  form of \textit{name binding} which can
  significantly complicate term rewriting.
Rewriting under binders,
  whether explicitly in the form of lambdas
  or implicitly with index notation,
  requires additionally analyzing the
  potential \textit{contexts}
  (what names may be bound to)
  of every subexpression.
While it is still technically possible to
  apply state-of-the-art rewrite engines
  like \tcd{egg}~\cite{willsey2021egg}
  via explicit variable substitution rules and
  free variable analyses,
  we have found the additional complexity
  and rewrite search space blow up
  substantially eliminate the potential advantages
  of term rewriting in such IR designs.

All the above constraints inform \g's key design goal:
  providing an IR that flexibly supports specifying and
  composing higher-order tensor operators\footnote{
    As \tcd{map} and \tcd{mapAt2} in 
    Section~\ref{subsec:pure-matmul} illustrate,
    an IR can support higher-order operators without
    necessarily providing lambdas, currying, or closures.}
  over arbitrary dimensions while still enabling
  high-performance term rewriting techniques
  like equality saturation.
In the rest of this paper,
  we show how \textit{access patterns} enable achieving
  these goals with a focus on applications to
  mapping application fragments down to
  specialized hardware accelerators.

%\hl{
%Need to be explicit somewhere that access patterns,
%especially things like cartesian product,
%can just be for specification -- you 
%do not necessarily have
%to materialize everything.}

%\hl{(Probably worth saying that rewrite systems need pure IRs. Maybe consider a phrasing like this --Steve) ``However, rewrite systems are generally defined over pure languages---how will we represent matrix multiplication in a pure IR?''}
  
%{\color{red} \noindent\rule{8.5cm}{10pt}}


% {\color{red} \begin{align*}
%     & \mcd{matMul(A, B) :=} \\
%     & \;\;\;\; \mcd{map(dotProd, cartProd(A, trans2(B)))}
% \end{align*} }

% $$ 
% {\color{red}
%   \mcd{cartProd : [f64] * [f64] -> [f64 * f64]}
% }
% $$




% %To understand
% %  the genesis
% %  of access patterns,
% %  let's walk through
% %  an example.
% Let us consider an example
%   to motivate 
%   the access pattern representation.
% %Imagine 
% %  we'd like to build
% %  a simple term rewriting system
% %  to map matrix multiplications
% %  to an accelerator.
% %  we've built.
% Suppose we would like
%   to use 
%   a simple term rewriting system 
%   to map matrix multiplications
%   to an accelerator.
% Before we can start building rewrites,
%   we need to
%   represent our programs of interest---%
%   beginning with matrix multiplication---%
%   in a pure IR.

% Recall 
%   the matrix multiplication
%   algorithm:
%   given two two-dimensional tensors
%   $a$ and $b$
%   with shapes
%   $(M, N)$
%   and
%   $(N, O)$,
%   respectively,
%   we take the dot product
%   of every row of $a$
%   with every column of $b$,
%   to produce a new tensor
%   with shape $(M, O)$.
% The English description
%   of the algorithm
%   immediately suggests
%   a pure
%   representation,
%   shown in
%   Figure \ref{fig:matmul-haskell} left.
% Assume
%   \texttt{a}
%   and \texttt{b}
%   are two-dimensional tensors
%   with
%   some tensor type.
% Here,
%   we use a simple implementation
%   for our tensor type:
%   lists of lists,
%   \texttt{[[f64]]}.
% \texttt{(rows a)}
%   and \texttt{(cols b)}
%   produce lists
%   of the rows of \texttt{a}
%   and the columns of \texttt{b},
%   respectively.
% Using our nested-lists
%   tensor type,
%   \texttt{rows}
%   is simply
%   the identity function,
%   and \texttt{cols}
%   is a transpose function.
% \texttt{cartProd}
%   returns every element 
%   of the first list
%   paired with every element
%   of the second list;
%   in this case, 
%   every row of \texttt{a}
%   paired with
%   every column of \texttt{b}.
% \texttt{dotProduct}
%   computes the dot product
%   of two vectors.
% Putting it all together,
%   we \texttt{map}
%   the \texttt{dotProduct}
%   over our row--column pairs,
%   producing a result
%   with type
%   \texttt{[f64]}.
  
% Something doesn't seem right about this.
% The result
%   of multiplying
%   two two-dimensional tensors
%   should be a 
%   two-dimensional tensor,
%   not a one-dimensional list.
% Our program
%   computes the right values,
%   but it loses
%  a key piece of information:
%   the shape
%   of the resulting tensor.
% % I'm not actually sure we need to explain why shape is important.
% %Preserving
% %  shape information
% %  in a tensor-based
% %  program
% %  is essential.
% %Shape
% %  is the de facto way
% %  in which type
% %  information
% %  is conveyed
% %  in many tensor-based programs.
% %Perhaps
% %  the most common example
% %  is in activation layouts:
% %  an activation layout
% %  of \texttt{NCHW},
% %  for example,
% %  conveys which dimension
% %  is the batch dimension
% %  (\texttt{N}),
% %  which dimension is the channel
% %  dimension
% %  (\texttt{C}),
% %  and which dimensions
% %  are the spatial dimensions
% %  (\texttt{H} and
% %    \texttt{W}),
% %  and their order.
% Let's understand
%   where
%   the shape information
%   was lost.
% Both
%   \texttt{rows}
%   and
%   \texttt{cols}
%   preserve shape information,
%   which we can see
%   from their type,
%   \texttt{[[f64]] -> [[f64]]}.
% What about
%   \texttt{cartProd}?
% Its type signature
%   is
%   \texttt{[f64] -> [f64] -> [(f64, f64)]};
%   given two lists,
%   it returns a list of tuples.
% This new list
%   is of length
%   \texttt{length l0 * length l1}
%   for lists
%   \texttt{l0} and \texttt{l1}.
% In our example,
%   \texttt{rows a}
%   is a list of length $M$
%   of lists of length $N$,
%   and \texttt{cols b}
%   is a list of length $O$
%   of lists of length $N$.
% Thus,
%   \texttt{cartProd}
%   produces a list of length
%   $M \cdot O$
%   of 2-tuples
%   containing lists of length $N$.
% By the time
%   we map
%   \texttt{dotProduct}
%   over this list,
%   the shape information
%   has already been lost;
%   \texttt{dotProduct}
%   simply produces
%   another list
%   of length $M \cdot O$,
%   but with scalar values.
% It would seem
%   shape information
%   is lost
%   by \texttt{cartProd}.

% What
%   were we expecting
%   the result 
%   of \texttt{cartProd}
%   to be?
% We expect
%   the result of
%   the entire \texttt{map} expression
%   to be a tensor
%   with shape
%   $(M, O)$;
%   that is,
%   the outermost dimensions
%   of the original two inputs
%   ($M$ and $O$, respectively)
%   need to be preserved
%   \textit{separately,}
%   rather than
%   being \textit{flattened}
%   into a single dimension
%   of length $M \cdot O$.
% %Thus,
% %  the result of
% %  \texttt{cartProd}
% %  should presumably
% %  have a shape like
% %  $(M, O, \dots)$,
% %  rather than
% %  $(M*O, \dots)$.
% This brings us
%   to our first core obstacle:
%   we are using
%   the
%   one-dimensional list
%   semantics
%   of common functions
%   such as
%   \texttt{cartProd},
%   when
%   we need more complex semantics
%   for our more complex
%   tensor type.
  
% As a first pass
%   at this obstacle,
%   we can redefine
%   \texttt{cartProd}
%   to preserve
%   shape information.
% Its type signature
%   will change
%   from
%   \texttt{[f64] -> [f64] -> [(f64, f64)]}
%   to
%   \texttt{[f64] -> [f64] -> [[(f64, f64)]]}.
% That is,
%   rather than producing
%   a single list
%   whose length is a product
%   of the input lists' lengths,
%   we will produce a list of lists,
%   or a two-dimensional tensor.
% This
%   preserves
%   our shape information.
  
% Now, though,
%   we are presented
%   with another problem;
%   how should \texttt{map}
%   work
%   over our tensor representation?
% Currently,
%   \texttt{map}'s type
%   is 
%   \texttt{(f64 -> f64) -> [f64] -> [f64]};
%   thus,
%   it will attempt to map
%   our uncurried
%   \texttt{dotProduct}
%   over only 
%   the \textit{outermost}
%   dimension
%   of the result of 
%   \texttt{cartProd}.
% Not only
%   is this not what we want---%
%   it is also 
%   a type error.
% By preserving
%   the shape information
%   through
%   \texttt{cartProd},
%   we have necessitated
%   a more complex
%   implementation
%   of \texttt{map}.
% Specifically,
%   \texttt{map}
%   needs to understand
%   which dimensions
%   of the input tensor
%   should be considered the
%   ``list'' dimensions,
%   unaffected by the map,
%   and which dimensions
%   should be considered
%   the ``item'' dimensions,
%   which represent
%   the data structure
%   being passed in
%   to the function.
% In this case,
%   we want
%   \texttt{map}
%   to map the function
%   over the tuples
%   contained in the second
%   level
%   of lists.
% Figure \ref{fig:matmul-haskell} right
%   shows
%   our updated 
%   matrix multiplication,
%   using a new function,
%   \texttt{tensorMap2},
%   which treats
%   tensor dimensions
%   0 and 1
%   as ``list'' dimensions
%   and maps a function
%   over dimension 2
%   and above.

% With our change to
%   \texttt{cartProd}
%   and our new version
%   of \texttt{map},
%   we finally have
%   a functional representation
%   of matrix multiplication.
% Using this representation,
%   we can start writing rewrite rules
%   for our hardware accelerator,
%   in terms of
%   \tcd{tensorMap2},
%   \tcd{dotProduct}, and
%   \tcd{cartProd}.
% However,
%   these operators
%   are defined
%   for our specific
%   two-dimensional
%   usecase,
%   and thus, the rewrite would be brittle;
%   if $a$ or $b$
%   had more dimensions,
%   we would again
%   need to redefine
%   \texttt{map},
%   \texttt{cartProd},
%   and our rewrite as a whole.
  
% \texttt{cartProd}
%   and
%   \texttt{tensorMap2}
%   are conveying
%   which dimensions
%   of the input tensors
%   are being \textit{iterated over}
%   and which are
%   being \textit{computed on}.
% But instead
%   of conveying
%   this information
%   in these functions'
%   types,
%   it
%   could instead
%   be conveyed 
%   by the type
%   of the tensor itself.
% This is the core insight
%   of access patterns,
%   which we will now describe.
  
\section{\g}
\label{sec:glenside}
  
% Trying to get this to end up on the right page...
\begin{table*}
    \centering
    \begin{tabularx}{\linewidth}{lXX}
    Transformer 
    & Input(s)
    & Output Shape  \\
    \hline
    
    %%%%%% ACCESS
    \texttt{access} 
    &
    \accesspatternshape{a_0,\dots}{\dots, a_n}
    and nonzero integer $i$
    & 
  \accesspatternshape
  {a_0, \dots, a_{i-1}}{a_i,\dots, a_n}
    \\
    
    \texttt{cartProd} 
    &
    \accesspatternshape{a_0,\dots, a_n}{c_0, \dots, c_p} \newline
    and \accesspatternshape{b_0,\dots, b_m}{c_0, \dots, c_p}
    & 
  \accesspatternshape
  {a_0, \dots, a_n, b_0,\dots, b_m}
  {2, c_0, \dots, c_p}
    \\
    %...implementing the rows-by-columns data reading pattern used in matrix multiplication; implementing the filter--window pairing in \ctd{} \\
    
    %%%% WINDOWS
    \texttt{windows} 
    &
    \accesspatternshape{a_0, \dots, a_m}{b_0, \dots, b_n}, \newline
    window shape $(w_0, \dots, w_n)$,
    strides $(s_0, \dots, s_n)$
    &
    \accesspatternshape{a_0, \ldots, a_m, b'_0, \dots, b'_n}{w_0, \dots, w_n},\newline
    where $b'_i = \lceil (b_i - (k_i - 1)) / s_i \rceil $\\
    
    %%%% SLICE
    \texttt{slice} &
    \accesspatternshape{a_0, \dots }{\dots, a_n}, \newline
    dimension index $d$, bounds $[l, h)$
    &
    \accesspatternshape{a'_0, \dots }{\dots, a'_n} \newline
    with $a'_i = a_i$ except $a_d = h - l$
    \\
    %...accessing a subset of a tensor \\
    
    \texttt{squeeze} &
    \accesspatternshape{a_0, \dots }{\dots, a_n}, \newline
    dimension index $d$; we assume $a_d = 1$
    &
    \accesspatternshape{a_0, \dots }{\dots, a_n}\newline
    with $a_d$ removed
    \\
    
    \texttt{flatten} &
    \accesspatternshape{a_0,\dots,a_m}{b_0,\dots,b_n} &
    \accesspatternshape{a_0 \cdots a_m}{b_0 \cdots b_n} \\
    
    \texttt{reshape} &
    \accesspatternshape{a_0,\dots,a_m}{b_0,\dots,b_n},\newline
    access pattern shape literal
    \accesspatternshape{c_0,\dots,c_p}{d_0,\dots,d_q}&
    
    \accesspatternshape{c_0,\dots,c_p}{d_0,\dots,d_q},\newline
    if $a_0 \cdots a_m = c_0 \cdots c_p$
    and $b_0 \cdots b_n = d_0 \cdots d_q$\\
    
    \end{tabularx}
    \caption{\g's access pattern transformers.}
    \label{tab:access-pattern-transformers}
\end{table*}

%\hl{Move up resolution of section 3 to beginning of section 4}

%\hl{max: do a bigger bomb drop. what have we done before we compute dot product? we've set up a complex system of accessing, which is completely separate from the computation that we're doing}

%\hl{the interesting thing is that these kernels end up looking very similar when you phrase them in glenside}

%\hl{need to say that we're solving the problem we set up}

%\hl{see, we've done what we said we're going to do}

This section details \g's implementation,
  focusing on its core abstraction,
  \textit{access patterns},
  and how they can be composed and computed over.
We continue using Section~\ref{sec:matmul}'s
  matrix multiplication as a
  running example throughout.
  %beginning
  %with the definition
  %of access patterns,
  %then ways we can combine access patterns,
  %and finally,
  %describing how we compute over access %patterns.
%To illustrate,
   %we will progressively convert
   %the matrix multiplication example
   %from Section~\ref{sec:matmul}
   %to our final \g implementation.
  

\subsection{Access Patterns}

% Access patterns
%   encode a common trope
%   in tensor programs
%   in which
%   some of the dimensions of a tensor
%   are \textit{iterated over}/\textit{accessed}
%   while others are
%   \textit{computed on.}

Access patterns encode common
  tensor IR patterns where
  some tensor dimensions
  are \textit{iterated over} (accessed)
  while others are \textit{computed on}.
Section~\ref{sec:matmul}'s \tcd{matMul} example
  \textit{iterates over} dimension 0 of input $P$,
  while \textit{computing on} dimension 1,
  effectively viewing $P$ as a 1D vector of 1D vectors.

Access patterns are specified by their \textit{shape} ---
  a pair of tuples of positive integers $(S_A, S_C)$.
An access pattern of shape $(S_A, S_C)$ is in turn a
  tensor $T$ whose shape is given by the
  concatenation of the access pattern shape tuples,
  $S_A \,\mcd{++}\, S_C$; we call
  $S_A$ the \textit{access} and
  $S_C$ the \textit{compute}
  dimensions of $T$ respectively.

Access patterns represent the view of an
  $(|S_A| + |S_C|)$-dimensional tensor
  as a tensor of shape $S_A$,
  each of whose elements has shape $S_C$.
For an access pattern $T$ of shape $(S_A, S_C)$
  where $|S_A| = n_A$, we use the syntax
  \tcd{(access $T$ $n_A$)} to represent $T$ in \g.
For example, if a 2D matrix $T$ has shape $(m, n)$,
  then the \g expression \tcd{(access $T$ 1)}
  yields an access pattern of shape $((m), (n))$.
  

% % We formally define access patterns
% %   as follows:
% An access pattern is defined by its
%   \textit{shape} ---
%   a pair of tuples
%   \accesspatternshape
%     {s_0, \dots, s_{m-1}}
%     {s_{m}, \dots, s_n}
%   where $s_i \in \mathbb{Z^+}$.
% An access pattern of shape
%   \accesspatternshape
%     {s_0, \dots, s_{m-1}}
%     {s_{m}, \dots, s_n}
%   is a tensor $t$
%   of shape
%   \[(s_0, \dots, s_{m-1},s_{m}, \dots, s_n)
%   \]
%   where $(s_0, \dots, s_{m-1})$
%   are called
%   its \textit{access} dimensions
%   and $(s_{m}, \dots, s_n)$
%   are called 
%   its \textit{compute} dimensions.
% The access pattern
%   represents the interpretation of $t$
%   as a tensor of shape
%   $(s_0, \dots, s_{m-1})$,
%   where every element
%   is a tensor
%   of shape
%   $(s_{m}, \dots, s_n)$.
% We call these
%   $(s_{m}, \dots, s_n)$-shaped
%   tensors
%   the access pattern's
%   \textit{subviews.}
% In \g,
%   we construct such an access  pattern
%   with the syntax
%   \texttt{(access t m)}.
% % With this definition,
% %   we now have the tools needed
% %   to represent
% %   our view of $P$
% %   in our matrix multiplication example.
% If a matrix $P$ has shape
%   $(M, N)$,
%   then the \g expression
%   \texttt{(access P 1)}
%   produces an access pattern
%   of shape
%   \accesspatternshape
%   {M}
%   {N}.
  
The matrix multiplication example
  from Section~\ref{sec:matmul}
  directly accesses the rows of $P$,
  but uses transpose to iterate over
  the columns of $Q$.
Instead of requiring an explicit
  transpose operator, \g provides
  access pattern \textit{transformers}.

\subsection{Access Pattern Transformers}

Access pattern transformers 
  manipulate one
  or more access patterns
  to produce a new access pattern,
  allowing \g to
  to support more complex patterns
  like
  slicing,
  transposing,
  and interleaving.
  Table~\ref{tab:access-pattern-transformers}
  lists \g's transformers.
  
% So far in our matrix multiplication example,
%   we have created
%   an access pattern
%   \texttt{(access P 1)}
%   representing the rows of $P$.
To produce an access pattern
  representing
  the columns of $Q$
  for matrix multiplication,
  we employ
  the \texttt{transpose}
  transformer.
It takes an access pattern
  and a list of dimension indices,
  and rearranges
  the dimensions 
  of the access pattern
  in the order specified by the indices.
Thus,
  \texttt{(transpose (access Q 1) (list 1 0))}
  swaps the position
  of the two dimensions;
  if $Q$ has shape $(N, O)$,
  this expression produces
  an access pattern
  of shape
  \accesspatternshape{O}{N}.
  
% Next,
%   matrix multiplication
%   requires
%   we implement
%   a Cartesian product.
% Section~\ref{sec:matmul} described the
%   challenges of representing
%   Cartesian products.
%   gave us trouble
%   under array-of-array
%   semantics
%   in \autoref{sec:matmul}.
% We were forced to define
%   \tcd{cartProd2D},
%   which was hard-coded
%   for our specific inputs
%   (2D tensors)
%   and our desired output
%   (pairs of the vectors in dimension 1
%     of the input tensors).
% However, access patterns
%   enable
%   a surprisingly intuitive 
%   and flexible definition.
The \texttt{cartProd} transformer
  takes access patterns
  of shapes
  \accesspatternshape{a_0, \dots, a_n}{c_0, \dots, c_p}
  and 
  \accesspatternshape{b_0, \dots, b_m}{c_0, \dots, c_p}
  respectively, and produces 
  an access pattern of the shape
  \accesspatternshape
    {a_0, \dots, a_n, b_0,\dots, b_m}
    {2, c_0, \dots, c_p},
  where $(2, c_0, \dots, c_p)$
  represents a 2-tuple
  of the input access patterns'
  compute dimensions.
The access dimensions
  of the input access patterns
  are simply concatenated.
In the matrix multiplication example,
  the Cartesian product
  of the rows of $P$
  with the columns of $Q$
  is an access pattern
  of shape
  \accesspatternshape{M,O}{2, N},
  where the second shape
  represents a 2-tuple
  of a row from $P$
  with a column from $Q$.

We have nearly re-implemented
  matrix multiplication example
  in \g.
The final step
  is to compute dot product, for which
  \g uses 
  access pattern \textit{operators}.
  
\subsection{Access Pattern Operators}

\begin{table}
    \centering
    \begin{tabularx}{\linewidth}{lXX}
    Operator & Type & Description\\
    \hline
    \texttt{reduceSum} & $(\dots) \rightarrow ()$ &
    sum values
    \\
    
    \texttt{reduceMax} & $(\dots) \rightarrow ()$&
    max of all values\\
    
    \texttt{dotProd} &
    $(t,s_0, \dots, s_n)\rightarrow ()$ &
    eltwise mul; sum
    %eltwise.~mult.~$t$ tensors  of  shape $(s_0, \dots, s_n)$, reduce with  sum
    \\
    
% COPIED right from glenside source. just add to table as needed!
%        DotProduct,
%    ReduceSum,
%    ReLU,
%    Sqrt,
%    Negative,
%    /// Expects item shape of `a x b1 x .. x bn`. Performs an elementwise
%    /// addition of the `a` tensors of size `b1 x .. x bn`.
%    /// TODO(@gussmith) Multiple-arg compute feels clunky and ad-hoc.
%    /// Should figure out an explicit way to define access multiple-stream
%    /// access patterns.
%    ElementwiseAdd,
%    /// Expects item shape of `a x b1 x .. x bn`. Performs an elementwise
%    /// multiplication of the `a` tensors of size `b1 x .. x bn`.
%    ElementwiseMul,
%    ElementwiseDiv,
%    /// Takes the max across all elements in each item. Reduces any item shape
%    /// to a scalar.
%    ReduceMax,
%    /// Computes softmax. Currently expects access axis to be 0. Unsure how to
%    /// define softmax for other access patterns.
%    Softmax,
%    /// For an item shape of `a1 x a2 x ...`, returns an item shape of `1` where
%    /// the returned scalar is the mean of the `a1 x a2 x ...`-shaped tensor.
%    ReduceMean,

   
    \end{tabularx}
    \caption{\g's operators.}
    \label{tab:operators}
\end{table}

\textit{Operators}
  are the only \g
  constructs
  which
  perform computation.
They are invoked only
  in \texttt{compute} expressions,
  which map the operator
  over the compute dimensions
  of an access pattern.
For an input access pattern
  of shape
  \accesspatternshape
  {s_0, \dots, s_{m-1}}
  {s_m, \dots, s_{n}},
  and an operator
  with type
  $(s_m,\dots,s_n)
  \rightarrow
  (s'_{m'}, \dots, s'_{n'})$,
  the result of
  \texttt{(compute <op> <accessPattern>)}
  will have shape
  \accesspatternshape
  {s_0, \dots, s_{m-1}}
  {s'_{m'}, \dots, s'_{n'}};
  that is, a \tcd{compute}
  expression
  cannot change
  the access dimensions
  of the input access pattern.
Table \ref{tab:operators}
  lists
  the operators
  in \g{}.
  
Recall where we are
  in converting
  our matrix multiplication
  example:
  we have accessed the rows of $P$
  and the columns of $Q$
  and taken their Cartesian product,
  resulting in an access pattern
  of shape
  \accesspatternshape
  {M, O}{2, N},
  and we need now
  to compute the dot product
  of these row--column
  pairs.
In \g,
  the \texttt{dotProd}
  operator
  (see Table~\ref{tab:operators})
  does just that.
To compute the dot product
  over our row--column pairs,
  we need only to apply
  \texttt{compute dotProd}
  to our access pattern,
  to produce an access pattern
  with final shape
  \accesspatternshape
  {M, N}{}.
The entire \g
  implementation
  of matrix multiplication
  is shown in Figure \ref{fig:mat-mat-mult}.
  
\definecolor{gray}{Gray}{5} 
  
\begin{figure*}
\begin{minipage}{.54\textwidth}
\begin{subfigure}{\textwidth}
\begin{lstlisting}[escapechar=!]
(transpose                   !\color{gray}; \hspace{2mm}\accesspatternshape{N, O, H', W'}{}!
 (squeeze                    !\color{gray}; \hspace{2mm}\accesspatternshape{N, H', W', O}{}!
  (compute dotProd           !\color{gray}; \hspace{2mm}\accesspatternshape{N, 1, H', W', O}{}!
   (cartProd                 !\color{gray}; \hspace{2mm}\accesspatternshape{N, 1, H', W', O}{2, C, K_h, K_w}!
    (windows                 !\color{gray}; \hspace{2mm}\accesspatternshape{N, 1, H', W'}{C, K_h, K_w}!
     (access activations 1)  !\color{gray}; \hspace{2mm}\accesspatternshape{N}{C,H,W}!
     (shape C Kh Kw)
     (shape 1 Sh Sw))
    (access weights 1)))     !\color{gray}; \hspace{2mm}\accesspatternshape{O}{C, K_h, K_w}!
  1)
 (list 0 3 1 2))
 
     \end{lstlisting}
       \vspace{-1.5em}
    \subcaption{2D convolution. Activations in \texttt{NCHW} format; weightsin \texttt{OIHW} format. }
    \label{fig:conv2d}
\end{subfigure}
\end{minipage}
\begin{minipage}{.45\textwidth}

%%%%%% MATMUL
\begin{subfigure}{\textwidth}
\begin{lstlisting}[escapechar=!]
(compute dotProd          !\color{gray}; \hspace{2mm}\accesspatternshape{M, O}{}!
 (cartProd                !\color{gray}; \hspace{2mm}\accesspatternshape{M, O}{2, N}!
  (access activations 1)  !\color{gray}; \hspace{2mm}\accesspatternshape{M}{N}!
  (transpose              !\color{gray}; \hspace{2mm}\accesspatternshape{O}{N}!
   (access weights 1)     !\color{gray}; \hspace{2mm}\accesspatternshape{N}{O}!
   (list 1 0))))
  \end{lstlisting}
  \vspace{-1.5em} 
  \subcaption{Matrix multiplication.}
  \label{fig:mat-mat-mult}
\end{subfigure}

%%%%% MAXPOOL
\begin{subfigure}{\textwidth}
\begin{lstlisting}[escapechar=!]
(compute reduceMax       !\color{gray}; \accesspatternshape{N,C,H',W'}{}!
 (windows                !\color{gray}; \accesspatternshape{N,C,H',W'}{K_h, K_w}!
  (access activations 2) !\color{gray}; \accesspatternshape{N, C}{H, W}!
  (shape Kh Kw)
  (shape Sh Sw)))
\end{lstlisting}
  \vspace{-1em} 
  \subcaption{Max pooling.}
  \label{fig:maxpool-code}
\end{subfigure}

\end{minipage}
\caption{Common tensor kernels from machine learning expressed in \g. Lines containing access patterns are annotated with their access pattern shape.
$N$ is batch size; $H$/$W$ are spatial dimension sizes; $C$/$O$ are input/output channel count; $K_h$/$K_w$ are filter height/width; $S_h$/$S_w$ are strides.
}
\label{fig:all-kernels}
\end{figure*}

\section{Case Studies}
\label{sec:case-studies}

% Unsure where this should go for now
%\begin{figure}
%\begin{lstlisting}[escapechar=!]
%matrix multiplication:
%!\colorbox{cyan}{(compute dotProd}!
% !\colorbox{cyan}{(cartProd}!
%  (access a 1)
%  (transpose (access b 1) (list 1 0))!\colorbox{cyan}{))}!
%    ...map to hardware...
%!\colorbox{CarnationPink}{(systolic-array ?rows ?cols}!
% (access a 1)
% (access b 1)!\colorbox{CarnationPink}{)}!
%  
%  
%conv2d:
%!\colorbox{cyan}{(compute dotProd}!
% !\colorbox{cyan}{(cartProd}!
%  (access weights 1)
%  (windows (access activations 4) ...)!\colorbox{cyan}{))}!
%      ...im2col...
%!\colorbox{cyan}{(compute dotProd}!
% !\colorbox{cyan}{(cartProd}!
%  (access weights 1)
%  (flatten 
%   (windows (access activations 4) ...))!\colorbox{cyan}{))}!
%    ...map to hardware...
%!\colorbox{CarnationPink}{(systolic-array ?rows ?cols}!
% (access weights 1)
% (flatten
%  (windows (access activations 4) ...))!\colorbox{CarnationPink}{)}!
%\end{lstlisting}
%
%    \caption{
%    \hl{TODO hideous. make fixed width, make smaller than text. unsure where this should go}
%    \g{}'s access pattern
%      data structure
%      reveals the similarity
%      between
%      dense matrix multiplication
%      and 2D convolution,
%      and elegantly leads
%      to the discovery
%      of the \tcd{im2col}
%      transform,
%      allowing both kernels
%      to be mapped
%      to the same hardware.}
%       %zach's case for figure 5: it's context free understandable. we need to go back to somethign more visual. make a version that someone can get with 5 seconds of viewing,and then maybe has more information for someone with a minute of viewing.
%    \label{fig:blah}
%\end{figure}

To demonstrate
  the utility of \g,
  we show how it expresses
  useful ML kernels
  (\ref{section:representing-kernels}).
We then use
  equality saturation
  (provided by the \texttt{egg} library~\cite{willsey2021egg})
  to implement a term rewriting system
  over \g, to
  map matrix multiplications
  to systolic arrays
  (Section \ref{sec:case-study-tensorization}),
  run 2D convolutions on matrix multiplication hardware
  (Section \ref{sec:discovering-im2col}),
  and discover
  matrix multiplication
  blocking schemes
  (Section \ref{sec:case-study-blocking}).
\g is implemented in ~25k lines of
  Rust.\footnote{Publicly available at [redacted for anonymous submission].}
  
  
%\subsection{Representation of Common Machine Learning Kernels}
\subsection{Representation of Common ML Kernels}
\label{section:representing-kernels}

Figure~\ref{fig:all-kernels}
  lists the \g implementations
  of three common ML kernels:
  2D convolution,
  matrix multiplication,
  and max pooling.
Below, we discuss
  their implementations in detail
  (matrix multiplication,
  is shown in Section~\ref{sec:glenside}).
  
\subsubsection{2D Convolution}

2D convolution (\ctd{})
  is a core kernel
  in deep learning,
  defined element-by-element %(as in the TVM and TensorFlow documentation)
  over tensors storing
  activations $A$,
  strides $S$, and
  weights $W$ as: 
  % (from the Relay docs \cite{relay-conv2d}):
  % note: TF's tf.nn.conv2d gives almost an identical definition
\begin{equation*}%\label{eq:conv2d}
\begin{split}
\mbox{out}&[n, o, x, y] =\\
\sum_{dx, dy, c}&
    (A[n, c, S[0] \cdot x  + dx, S[1] \cdot y + dy] \
    \cdot W[o, c, dx, dy])
\end{split}
\end{equation*}
where
  $n$ indexes the output batch,
  $o$ indexes output channels,
  $x$/$y$ index spatial dimensions,
  $dx$/$dy$ index
    the convolutional window spatial dimensions,
  and $c$ indexes input channels.
%Written this way,
%  we can see that
%  each output location
%  $\mbox{out}[n,c,y,x]$,
%  is
%  the result of
%  a ``generalized'' dot product
%  of a three-dimensional window
%  of the ``data'' input
%  with one of the $c$ 
%  three-dimensional filters
%  in the ``weight'' input.
%Computationally,
%  a generalized dot product
%  is straightforward:
%  it's a pairwise multiplication
%  between two sets of numbers
%  followed by 
%  a reduction sum.
%Most of the complexity
%  of the \ctd{} kernel
%  is 
%  not in the computation itself,
%  but
%  in
%  how the data
%  is \textit{accessed:}
%  how we correctly order the values
%  to be multiplied and accumulated.
%\ctd{}
%  % TODO mention that padding also happens here?
%  must first
%  access the ``data'' input
%  as windows,
%  respecting the kernel height,
%  kernel width,
%  and stride parameters.
%It then
%  takes the
%  Cartesian product
%  of the windows
%  with
%  the list of filters in ``weight''.
%Additionally,
%  not shown in equation
%  \ref{eq:conv2d},
%  it's also common
%  to apply padding
%  to the input data,
%  which can be done
%  as the data is being accessed.
%It is only once
%  these accesses are set up
%  that we are able
%  to map
%  the generalized dot product
%  to calculate the result.
2D convolution
  slides each of the $o$
  filters
  of shape $(c, dx, dy)$
  through each possible
  $(c, dx, dy)$-shaped window
  of the input images.
At each of these locations,
  an element-wise multiplication
  and reduction sum
  is computed.

The \g implementation
  of \ctd{}
  is shown in 
  Figure \ref{fig:conv2d}.
We access
  the \texttt{weights}
  as a vector of $O$ filters
  and the \texttt{activations}
  as a vector of $N$ images.
We leave the filters as they are,
  but form windows
  of shape
  $(C, K_h, K_w)$
  over the activations
  using the \texttt{windows}
  access pattern transformer
  (Table~\ref{tab:access-pattern-transformers}).
This produces an access pattern
  of shape
  \accesspatternshape
  {N, 1, H', W'}
  {C, K_h, K_w},
  i.e.,
  a batch of ``images''
  of new spatial shape
  $(H', W')$
  where every location
  is a window of
  the original input.
Finally,
  we take the Cartesian product
  of the filters
  and the windows,
  compute their dot product,
  and \texttt{squeeze} and \texttt{transpose}
  the output
  into the correct layout.
  
%There is very little
%  computation
%  represented
%  in the \g{} code---%
%  only the outermost 
%  \texttt{compute dotProd}.
%Most of the complexity
%  is in expressing
%  exactly how the large,
%  multi-dimensional tensors
%  are accessed.
%Surprisingly,
%  the base structure
%  of \ctd{}
%  looks a lot like
%  our matrix--matrix multiplication
%  example:
%  we take the
%  Cartesian product
%  of two access patterns
%  and map a dot product
%  over the result.
%In matrix--matrix multiplication,
%  the two access patterns
%  access the rows of the first matrix
%  and the columns
%  of the second matrix,
%  respectively.
%What are
%  the two access patterns
%  in the case of
%  \ctd{}?
  

%In \g,
%  we construct
%  the convolutional window access pattern
%  with a nested series
%  of access pattern transformers.
%We begin
%  by converting the tensor
%  into an access pattern.
%We then pad
%  the access pattern
%  with \texttt{access-pad}.
%We add 1 length
%  of zero-padding
%  before and after
%  dimensions 2 and 3.
%Note that this
%  could also be done
%  as a pre-processing step.
%We access the padded tensor
%  at dimension 4,
%  which is simply to prepare it
%  to be input to
%  \texttt{access-windows}.
%At this point,
%  our access pattern
%  is of shape
%  \accesspatternshape
%  {1, 3, 34, 34}
%  {}.
%We then use
%  \texttt{access-windows},
%  which takes three arguments:
%  the access pattern,
%  the window shape,
%  and the strides.
%\texttt{access-windows}
%  expects an access pattern
%  of shape
%  \accesspatternshape{(a_0,\dots,a_n}{}.
%The window shape
%  $(w_0, \dots, w_n)$
%  and the strides
%  $(s_0, \dots, s_n)$
%  are the same length
%  as the access pattern shape.
%The resulting access pattern is of shape
%  \accesspatternshape{a'_0,\dots,a'_n}{s_0, \dots, s_n},
%  where $a'_i$
%  is determined by
%  the number
%  of
%  valid window locations,
%  given the input shape,
%  the window shape,
%  and the strides.
%Specifically, $a'_i$
%  is the number of locations
%  in which
%  we can lay down
%  the edge of the window
%  in dimension $i$,
%  given that the window
%  is length $w_i$ along dimension $i$,
%  and given that we are striding by
%  $s_i$ along dimension $i$.
%In our example,
%  our input access pattern
%  is of shape
%  \accesspatternshape{1, 3, 34, 34}{},
%  and our window shape and stride
%  are 
%  $(1, 3, 3, 3)$ and
%  $(1, 1, 1, 1)$,
%  respectively.
%Along the first (batch) dimension,
%  we can lay down a window
%  in exactly one location,
%  as the dimension is length 1,
%  and the window is of length 1.
%This is the same
%  for the second (channel) dimension,
%  where the dimension is length 3,
%  and the window
%  is of length 3.
%However, for the last two
%  (spatial, height and width)
%  dimensions,
%  we can lay down
%  a window edge
%  of length 3
%  in multiple places
%  along a dimension
%  of length 34:
%  specifically, 32 places.
%Thus,
%  our resulting access pattern
%  is
%  \accesspatternshape
%  {1, 1, 32, 32}
%  {1, 3, 3, 3}.
%
%To finish
%  preparing
%  the convolutional window
%  access pattern,
%  we ``squeeze''
%  (remove)
%  some of the dimensions
%  of length 1
%  produced by
%  \texttt{access-windows},
%  and re-access
%  the tensor
%  at dimension 3.
%The result
%  is an access pattern
%  with shape
%  \accesspatternshape
%  {1, 32, 32}{3, 3, 3}.
%  
%We now have
%  an access pattern
%  over the weights
%  of shape
%  \accesspatternshape
%  {8}{3, 3, 3}
%  and an access pattern
%  over the activations
%  of shape
%  \accesspatternshape
%  {1, 32, 32}
%  {3, 3, 3}
%Finally,
%  we take their Cartesian product---%
%  which has shape
%    \accesspatternshape
%    {8, 1, 32, 32}
%    {2, 3, 3, 3}---%
%  and map a dot product
%  over the result,
%  giving an access pattern
%  with shape
%    \accesspatternshape
%    {8, 1, 32, 32}
%    {}.
%Finally,
%  we transpose
%  the result
%  to give us
%  an access pattern
%  of shape
%  \accesspatternshape
%  {1, 8, 32, 32}{},
%  which is back in
%  \texttt{NCHW}
%  format.

\subsubsection{Max Pooling}

Max pooling, commonly used in ML
  to condense intermediate activations,
  is defined as:
\begin{equation*}%\label{eq:maxpool}
\begin{split} 
\mbox{out}&[n, c, x, y] =\\
\max_{dx, dy}&
           (\mbox{activations}[n, c,
                       \mbox{strides}[0] \cdot x  + dx,  
                       \mbox{strides}[1] \cdot y + dy])
\end{split}
\end{equation*}

Max pooling 
  slides a window
  of shape $(dx, dy)$
  over all possible locations
  within the spatial (i.e.~$x$ and $y$)
  dimensions.
At each window location,
  it reduces the window
  to a scalar
  with the $\max$ operator.
The \g implementation merely applies
  \texttt{reduceMax} 
  over each two-dimensional window.
  
%To implement max pooling
  %in \g,
  %we first access 
  %the activation tensor
  %as a two-dimensional matrix
  %of two-dimensional matrices.
%We form windows
  %over each of these matrices,
  %and finally,
  %we compute over each window
  %with the \texttt{reduceMax}
  %operator.
  
\subsubsection{Discussion}\label{section:kernel-implementation-discussion}

\g separates
  the \textit{computation}
  from the \textit{data access patterns}
  in these kernels,
  meanwhile exposing
  the simplicity of their computation---%
  and the relative complexity
  of their data access.
In all three kernels,
  the computation can be described
  with a single operator;
  most of the implementation
  entails
  setting up the data access pattern.

Furthermore,
  \g exposes similar structure
  between kernels;
  for example,
  both \ctd
  and matrix multiplication
  feature the expression
  \tcd{(compute dotProd (cartProd ...))}.
At their core, these kernels
  are performing the same computation,
  but with different patterns
  of data access.
In Section~\ref{sec:discovering-im2col},
  we exploit this similarity in structure
  when mapping kernels to hardware.
%As we'll see in 
  %Section~\ref{sec:discovering-im2col},
  %\g can do more
  %than just highlight
  %this similar structure---%
  %it can exploit it
  %when mapping to hardware.
  
These kernels stress the expressive power
  of access patterns.
Consider the use of 
  \tcd{windows}
  in \ctd
   and max pooling.
Both kernels
  form windows
  differently:
  \ctd forms three-dimensional
  windows
  over the channels, height, and width
  dimensions,
  while max pooling forms two-dimensional windows
  over the height and width.
Rather than passing configuration parameters to \tcd{windows},
  \g attaches this information to the tensors themselves.
  
%With standard tensors,
  %we would have to express
  %these different intents
  %with a set of configuration parameters
  %passed in to \tcd{windows}.
%But with access patterns,
  %we can actually express this
  %via the extra information
  %attached to the tensor itself.
  
\begin{figure}
\begin{lstlisting}[escapechar=!]
(compute dotProd (cartProd ?a0 ?a1)) !$\Longrightarrow$!
  (systolicArray ?rows ?cols
    ?a0 
    (access (transpose ?a1 (list 1 0)) 0))
!$\textrm{where \texttt{?a0} is of shape $((\mbox{\texttt{?batch}}), (\mbox{\texttt{?rows}}))$}$!
  !$\textrm{and \texttt{?a1} is of shape $((\mbox{\texttt{?cols}}), (\mbox{\texttt{?rows}}))$}$!
\end{lstlisting}
\justify
\caption{Our rewrite rewriting matrix multiplication to a systolic array invocation.}
    \label{fig:systolic-array-rewrite}
\end{figure}
  
% Trying to get this on the right page.
\begin{figure*}
\begin{lstlisting}[escapechar=!]
                                                    ?a !$\Longrightarrow$! (reshape (flatten ?a) ?shape) 
                     (cartProd (reshape ?a0 ?a0Shape)   
                               (reshape ?a1 ?a1Shape)) !$\Longrightarrow$! (reshape (cartProd ?a0 ?a1) ?shape)
                 (compute dotProd (reshape ?a ?shape)) !$\Longrightarrow$! (reshape (compute dotProd ?a) ?newShape)
\end{lstlisting}
\caption{Rewrites used in \itc case study (Section~\ref{sec:discovering-im2col}).}
\label{fig:im2col-rewrites}
\end{figure*}

%\subsection{Mapping Dense \tcd{matMul}Matrix Multiplication to Accelerators}
\subsection{Mapping Dense \tcd{matMul} to Accelerators}
\label{sec:case-study-tensorization}


\g can be used to uncover opportunities
  to invoke accelerator components.
  %to discover places in our program
  %where we can invoke
  %accelerator components.
Consider a 
  weight-stationary systolic array,
  a common matrix multiplication
  architecture.
  %(commonly used for this purpose).
  %,
  %a common architecture
  %for implementing
  %matrix multiplication,
  %in which a weight matrix
  %is loaded
  %into the array
  %and remains stationary
  %while the activation matrix
  %is streamed through.
  %which loads and fixes a weight matrix in %the array
  %as the activation matrix is streamed %through.
The shape of a
  rectangular
  weight-stationary
  systolic array
  is given by its
  number of rows and columns,
  $(r,c)$.
The functioning
  of a weight-stationary
  systolic array
  can be described as
  taking two lists
  of length-$r$ vectors
  (the activations
    and weights, respectively),
  pairing each vector
  from one list
  with each vector
  from the other,
  and computing a dot product
  over each pair.
The second list
  contains $c$ vectors,
  while the first
  can be of any length.
  
\g's purity
  allows us to implement this hardware mapping task
  using a term rewriting system,
  in which we rewrite a matching program pattern
  to an invocation of our systolic array.
Our rewrite is shown in 
  Figure~\ref{fig:systolic-array-rewrite},
  mimicking
  \tcd{egg}'s rewrite syntax.
Tokens starting with a question mark
  (such as \texttt{?a0} in 
  Figure~\ref{fig:systolic-array-rewrite})
  are variables in the pattern,
  bound by the left-hand side (LHS),
  and then used on the right-hand side (RHS).
\tcd{egg} also allows for
  conditions on rewrites,
  which we print below our rewrites.

To design our rewrite,
  we first must design
  the LHS
  to match program patterns
  that resemble the data access pattern
  and compute pattern
  of our systolic array.
\g is eminently suitable for this task,
  as it can express
  %exposes
  %the perfect level
  %of abstraction
  %for this task,
  %allowing us to encode
  exactly the data access
  and computation pattern
  we described
  for the systolic array.
Pairing all vectors from one list
  with all vectors from another
  and computing the dot product
  of the pairs
  is represented as
  \tcd{(compute dotProd (cartProd ?a0 ?a1))},
  binding
  \tcd{?a0}
  and \tcd{?a1}
  to the input access patterns.
We encode
  the limitations
  of our systolic array
  on the input shapes
  as a condition on the rewrite.
Patterns which match the LHS
  are mapped to the RHS;
  in this case, we introduce a new
  \tcd{systolicArray} construct
  to represent the functioning of our systolic array.
The shape of the systolic array 
  is given by the \tcd{?rows} and \tcd{?cols}
  parameters,
  and the inputs are given
  as access patterns.
Note how we also transform
  the second access pattern
  to more accurately convey
  how the actual systolic array
  hardware
  accesses the weight tensor:
  it reads it all at once
  (hence \tcd{(access ... 0)}),
  and expects it laid out
  in its transposed form
  in memory.
This added information---%
  enabled by \g's access patterns---%
  provides richer data layout information,
  potentially helping future rewrites
  or code generation steps.
 
\subsection{Discovering \itc{}}\label{sec:discovering-im2col}
%\begin{figure}
%   \centering
%\begin{lstlisting}[escapechar=!]
%?a !$\Longrightarrow$! (reshape (flatten ?a) ?shape)
%!$\textrm{where \texttt{?shape} is the shape of \texttt{?a}}$!
%\end{lstlisting}
%\caption{Exploratory flatten--reshape rewrite.}
%\label{fig:flatten-unflatten}
%\end{figure}

%\begin{figure}
%\begin{lstlisting}[escapechar=!]
%(cartProd
% (reshape ?a0 ?a0Shape)
% (reshape ?a1 ?a1Shape))
%     !$\Longrightarrow$! (reshape (cartProd ?a0 ?a1) ?newShape)
%!$\textrm{where \texttt{?newShape} is the shape of the LHS \texttt{cartProd}}$!
%
%(compute dotProd (reshape ?a ?shape))
%     !$\Longrightarrow$! (reshape (compute dotProd ?a) ?newShape)
%!$\textrm{where \texttt{?newShape} is the shape of the LHS \texttt{compute dotProd}}$!
%\end{lstlisting}
%\caption{Composition commutativity of \texttt{reshape} with \texttt{cartProd} and \texttt{compute dotProd}.}
%\label{fig:reshape-composition-commutative}
%\end{figure}

\begin{figure}
\begin{lstlisting}[escapechar=!]
(transpose                   
 (squeeze                    
  (reshape           !\color{gray}; \hspace{2mm}\accesspatternshape{N,1,H',W',O}{}!
   (compute dotProd  !\color{gray}; \hspace{2mm}\accesspatternshape{N \cdot 1 \cdot H' \cdot W',O}{}!          
    (cartProd                 
     (flatten        !\color{gray}; \hspace{2mm}\accesspatternshape{N \cdot 1 \cdot H' \cdot W'}{C \cdot K_h \cdot K_w}!
      (windows                 
       (access activations 1)  
       (shape C Kh Kw)
       (shape 1 Sh Sw)))
     (flatten        !\color{gray}; \hspace{2mm}\accesspatternshape{O}{C \cdot K_h \cdot K_w}!
      (access weights 1))))
   ?shape)
   1)
 (list 0 3 1 2))
     \end{lstlisting}
     \vspace{-1em}
    \caption{An \tcd{im2col}-transformed 
      \ctd.
    This is the state of \ctd
    after the application of the flatten--unflatten and composition commutativity rewrites,
    and just before the application
    of the systolic array rewrite.
    The shapes of crucial subexpressions noted on the right.}
    \label{fig:conv2d-im2col-rewritten}
\end{figure}
  
The \tcd{im2col} transformation
  is a data layout optimization
  which enables computing \ctd
  on matrix multiplication hardware.
The transformation
  involves instantiating
  the convolutional windows
  over the input activations
  directly in memory~\cite{im2col}.
This leads to data duplication,
  but the resulting speedup
  more than offsets that overhead.
%  but the time and space overhead of duplication
%  is often negligible
  %compared to the speedup that running on 
  %matrix multiplication hardware
  %provides.
In this case study,
  we show how a few
  general rewrites
  within \g
  lead to the 
  \textit{automatic re-derivation}
  of the 
  \tcd{im2col} transformation.

%As  discussed
  %in section 
  %\ref{section:kernel-implementation-discussion},
  %\g exposes
  %striking similarities
  %in the structure
  %of \ctd
  %and matrix multiplication.
%We can also see this similarity
  %reflected in the fact that
  %\ctd
  %is similar
  %to the left-hand side
  %of our systolic array rewrite
  %in Figure \ref{fig:systolic-array-rewrite};
  %in fact, we can see 
  %the \tcd{(compute dotProd (cartProd ...))}
  %within \ctd.
\g's representation underscores
  the structural similarity
  between \ctd and matrix multiplication,
  reflected also by the shared
  \tcd{(compute dotProd (cartProd ...))}
  between \ctd 
  and the LHS of the systolic array rewrite
  in Figure~\ref{fig:systolic-array-rewrite}.
Using this rewrite 
  on \ctd would permit mapping it to the systolic array; 
  however, 
%But when we attempt
  %to apply this rewrite
  %to our current implementation
  %of \ctd,
  %we find that 
  the restrictions
  on the shape of
  \texttt{?a0}
  and \texttt{?a1}
  prevent that.
The systolic array has 
  an activation access pattern
  of shape \accesspatternshape
  {a}{b}
  and a weight access pattern
  of shape \accesspatternshape
  {c}{d},
  while \ctd operates over
  access patterns
  of shape \accesspatternshape
  {N, 1,H',W'}{C, K_h, K_w}
  and
  of \accesspatternshape
  {O}{C, K_h, K_w},
  respectively.
%We described our systolic array
  %as reading lists of vectors
  %as inputs:
  %that is, an activations access pattern
  %of shape \accesspatternshape
  %{a}{b}
  %and a weight access pattern
  %of shape \accesspatternshape
  %{c}{d},
  %while the \tcd{cartProd}
  %in
  %\ctd is operating over
  %higher-dimensional access patterns
  %of shape \accesspatternshape
  %{N, 1,H',W'}{C, K_h, K_w}
  %and
  %of \accesspatternshape
  %{O}{C, K_h, K_w},
  %respectively.
Coercing the access pattern
  into a lower-dimensional form
  would enable the systolic array rewrite.
  
\g uses
  an \textit{exploratory} rewrite,
  which the underlying rewrite engine
  optimistically applies
  to all access pattern expressions in the program,
  to convert the higher-dimensional access patterns
  of \ctd
  into the access patterns
  matched by the systolic array rewrite.
Simultaneously, it apply rewrites to
  clean up and shuffle
  the expressions introduced by the exploratory rewrite.
Eventually,
  these rewrites working in concert
  make the systolic array rewrite
  applicable,
  mapping \ctd
  to the systolic array.
%Next, we detail the rewrites
  %which enable this chain of events.
  
Figure~\ref{fig:im2col-rewrites} (first rule)
  shows this exploratory rewrite.
It flattens 
  an access pattern's shape
  and immediately reshapes it
  back to its original shape, preserving equality
(See Table~\ref{tab:access-pattern-transformers}
  for formal definitions).
The LHS matches
  any access pattern expression,
  permitting it to be used throughout the program
  potentially enabling further rewrites---hence, \textit{exploratory}.
%Hence, it is an \textit{exploratory} rewrite: 
  %it is used
  %throughout the program,
  %in hopes of enabling future rewrites.
However, the \tcd{reshape} operator
  will still need to be moved 
  in order to flatten the inputs to \tcd{cartProd}.
%However,
  %this rewrite by itself
  %does not do much,
  %as it does not change the shape
  %of the matched access pattern.
%If our goal
  %is to flatten
  %the inputs to \tcd{cartProd}
  %in our \ctd{} kernel,
  %we'll need to move the resulting
  %\tcd{reshape} operator around.

The second and third rewrites
  in Figure~\ref{fig:im2col-rewrites}
  %\ref{fig:concat-commutativity-rewrites}
  implement \textit{composition commutativity}
  of \tcd{reshape}
  with \tcd{cartProd} and \tcd{compute dotProd},
  which ``bubble'' \tcd{reshape} operators
  up and out of expressions.
These rewrites
  express general properties of these operators
  and are not specific
  to this task.

These three rewrites work in concert 
  to map \ctd
  to a systolic array
(since equality saturation 
  explores rewrites non-destructively, 
  the rewriting order here
  is purely for explanatory purposes) ---
first,
  the exploratory rewrite
  flattens and reshapes
  all access pattern expressions.
This includes the inputs
  to \ctd's \tcd{cartProd}
  subexpression,
  which are flattened
  to shape
  \accesspatternshape
  {N \cdot 1 \cdot H' \cdot W'}{C \cdot K_h \cdot K_w}
  and
  \accesspatternshape
  {O}{C \cdot K_h \cdot K_w}
  and reshaped
  back to their original shapes.
Next,
  the composition commutativity rewrites
  for \tcd{cartProd} 
  and
  \tcd{compute dotProd}
  fire in sequence,
  bubbling the \tcd{reshape} up
  through the 
  \tcd{cartProd} 
  and \tcd{dotProd} 
  expressions (shown in Figure~\ref{fig:conv2d-im2col-rewritten}).
Finally,
  the systolic array rewrite
  completes the \tcd{im2col} transform.
\g's equality saturation based rewrite engine
  discovers these rewrites
  because the exploratory rewrite 
  fires on every term
  and no rewrites are missed
  due to the absence of phase ordering.

This example highlights how,
  \textit{with straightforward,
  generally applicable rewrites
  defined over \g},
  equality saturation
  can emergently discover useful transformations
  that previously required
  expert insights to apply.
  %we can 
  %leverage the power of equality saturation
  %to produce interesting emergent transformations.

\subsection{Systolic Array Blocking}
\label{sec:case-study-blocking}

%\begin{figure}
%\begin{lstlisting}[escapechar=!]
%?a !$\Longrightarrow$! (concat (slice ?a ?dim ?b0 ?b1)
%               (slice ?a ?dim ?b1 ?b2) ?dim)
%\end{lstlisting}
%\justify
%\caption{
%Exploratory slice--concatenate rewrite.
%}
%\label{fig:slice-concat}
%\end{figure}

%\begin{figure*}
%\begin{lstlisting}[escapechar=!]
%                                  ?a !$\Longrightarrow$! (concat (slice ?a ?dim ?b0 ?b1) (slice ?a ?dim ?b1 ?b2) ?dim)
% (cartProd (concat ?a0 ?a1 ?dim) ?b) !$\Longrightarrow$! (concat (cartProd ?a0 ?b)           
%                                                 (cartProd ?a1 ?b) ?newDim)        if ?dim is an access dimension
%
%\end{lstlisting}
%\caption{Rewrites used in the matrix multiplication blocking case study (\autoref{sec:case-study-blocking}).}
%\label{fig:blocking-rewrites}
%\end{figure*}

\begin{figure}
\begin{lstlisting}[escapechar=!]
?a !$\Longrightarrow$! (concat (slice ?a ?dim ?b0 ?b1)
               (slice ?a ?dim ?b1 ?b2) ?dim)
               
(cartProd ?a (concat ?b0 ?b1 ?dim)) !$\Longrightarrow$!
 (concat (cartProd ?a ?b0) (cartProd ?a ?b1) ?newDim)
!$\textrm{if \texttt{?dim} is an access dimension}$!
 
(cartProd (concat ?a0 ?a1 ?dim0) 
          (concat ?a2 ?a3 ?dim1)) !$\Longrightarrow$!
 (concat (cartProd ?a0 ?a2) 
         (cartProd ?a1 ?a3) ?newDim)
!$\textrm{if \texttt{?dim0} and \texttt{?dim1} are the same shape dimension}$!

(compute dotProd (concat ?a0 ?a1 ?dim)) !$\Longrightarrow$!
 (concat (compute dotProd ?a0)
         (compute dotProd ?a1) ?dim)
!$\textrm{if \texttt{?dim} is an access dimension}$!

(compute dotProd (concat ?a0 ?a1 ?dim)) !$\Longrightarrow$!
 (compute reduceSum
  (pair (compute dotProd ?a0)
        (compute dotProd ?a1)))
!$\textrm{if \texttt{?dim} is a shape dimension}$!
\end{lstlisting}
\justify
\vspace{-1em}
\caption{
Rewrites for blocking \tcd{matMul}.
%blocking case study (\autoref{sec:case-study-blocking}).
}
\label{fig:all-blocking-rewrites}
\end{figure}

\begin{figure}
%!\color{gray}; \hspace{2mm}\accesspatternshape{M, O}{}!
\begin{lstlisting}[escapechar=!]
(concat                          !\color{gray}; \hspace{2mm}\accesspatternshape{32,32}{}!
 (concat                         !\color{gray}; \hspace{2mm}\accesspatternshape{16,32}{}!
  (compute reduceSum             !\color{gray}; \hspace{2mm}\accesspatternshape{16,16}{}!
   (pair                         !\color{gray}; \hspace{2mm}\accesspatternshape{16,16}{2}!
    (compute dotProd             !\color{gray}; \hspace{2mm}\accesspatternshape{16,16}{}!
     (cartProd                   !\color{gray}; \hspace{2mm}\accesspatternshape{16,16}{2, 16}!
      (slice                     !\color{gray}; \hspace{2mm}\accesspatternshape{16}{16}!
       (slice (access activations 1) 0 0 16) 1 0 16)
      (transpose                 !\color{gray}; \hspace{2mm}\accesspatternshape{16}{16}!
       (slice 
        (slice (access weights 1) 0 0 16) 1 0 16)
       (list 1 0))))
    (compute dotProd             !\color{gray}; \hspace{2mm}\accesspatternshape{16,16}{}!
     (cartProd                   !\color{gray}; \hspace{2mm}\accesspatternshape{16,16}{2, 16}!
      (slice                     !\color{gray}; \hspace{2mm}\accesspatternshape{16}{16}!
       (slice (access activations 1) 0 16 32) 1 0 16)
      (transpose                 !\color{gray}; \hspace{2mm}\accesspatternshape{16}{16}!
       (slice                    !\color{gray}; \hspace{2mm}\accesspatternshape{16}{16}!
        (slice (access weights 1) 0 16 32) 1 0 16)
       (list 1 0))))))
  ...
  \end{lstlisting}
  \vspace{-2em}
  \caption{A $32\times32$ \texttt{matMul}
  blocked into $16\times16$ \texttt{matMul}s
   via equality saturation per the rewrites in Figure
   \ref{fig:all-blocking-rewrites}.
   %\ref{fig:slice-concat} and %\ref{fig:concat-commutativity-rewrites}.
  The entire program, which totals 8 multiplications, is not shown.
  }
  \label{fig:matmul-rewritten}
  \vspace{-1em}
\end{figure}
Equality saturation 
  can also be used with \g 
  to emergently discover a
  systolic array
  blocking scheme.
%This case study will follow largely the same pattern
  %as \ref{sec:discovering-im2col}:
  %we will introduce an exploratory rewrite
  %and some associated ``cleanup'' rewrites,
  %and together they will produce the transformation
  %we are searching for.
Systolic array blocking
  is the common strategy
  of breaking up a single, large
  matrix multiplication
%  (or a 2D convolution
    %converted to a matrix multiplication)
  into smaller multiplications,
  by multiplying subsets
  of the input matrices
  and assembling the results
  to form the output matrix.
This is essential in practice,
  as systolic arrays have small, finite sizes
  (often between $16\times16$ and $256\times256$)
  but matrices in ML and HPC applications
  can be much larger.
%Blocking is essential,
  %as, in practice,
  %systolic arrays are generally
  %somewhere between
  %$16\times16$ and $256\times256$ in shape,
  %while matrix multiplications
  %and 2D convolutions
  %may be larger. % any possible cite?
%Indeed, many accelerators 
  %specifically facilitate blocking
  %with accumulator registers
  %for storing intermediate results.
%Blocking is so common
  %that matrix multiplication hardware
  %is generally fitted
  %with \textit{accumulators}:
  %buffers built to store
  %the temporary partial results
  %produced in the middle of a blocked
  %matrix multiplication.

As in Section~\ref{sec:discovering-im2col},
  this transformation follows
  from an exploratory rewrite
  and associated ``cleanup'' rewrites.
The exploratory rewrite used for blocking
  is shown at the top of Figure~\ref{fig:all-blocking-rewrites}.
Given an access pattern,
  this rewrite slices the access pattern
  into two pieces
  along a dimension
  and then concatenates them back together.
The dimension
  as well as the division strategy
  are configurable.
For this example,
  we assume for simplicity
  that we run this rewrite
  on every available dimension,
  that we divide each dimension
  perfectly in half,
  and that all dimensions are powers of 2 in size.
Figure~\ref{fig:all-blocking-rewrites} gives rewrites
  for bubbling the introduced
  \texttt{concat}
  operators up through the expression,
  namely the compositional commutativity
  of \tcd{concat}
  with \tcd{cartProd}
  and \tcd{compute dotProd}.
Starting from the matrix multiplication
  in Figure \ref{fig:mat-mat-mult},
  assuming input shapes of $(32,32)$,
  the exploratory rewrite first slices and concatenates
  the access patterns
  at the input of \tcd{cartProd}.
Then, using the commutativity rewrites,
  the resulting \tcd{concat}s
  are bubbled up
  to produce the final expression
  in Figure \ref{fig:matmul-rewritten} --- the effect of these rewrites
  is that the single 
  $32\times 32$ \tcd{matMul}
  becomes eight separate $16\times 16$ \tcd{matMul}s,
  which are
  summed
  and concatenated
  to form the full output matrix.
This case study demonstrates
  yet again
  that \g's expressiveness
  allows a small set
  of rewrites
  to produce interesting and useful
  emergent transformations.
  
% From here, we can apply
%   our systolic array rewrite (\autoref{fig:systolic-array-rewrite}).

    
%\section{Future Work and Conclusion}
\section{Conclusion}
\label{sec:conclusion}

In this paper,
  we proposed \textit{access patterns} as an
  abstraction to enable equality saturation
  style term rewriting for low-level
  tensor program mapping to hardware accelerators.
Crucially, access patterns support
  specifying and composing
  higher-order, arbitrary dimension
  tensor operators without the need
  for binding structures like anonymous functions
  or index notation.
We demonstrated the potential utility of
  access patterns in the \g IR through
  case studies showing how rewrites in \g
  can automatically uncover common
  layout transformations like \tcd{im2col}
  used for accelerator mapping.
We are excited for the community
  to join in further exploring the potential
  applications of access patterns and to
  build additional optimizations on
  \g's foundations.

%only a small number of giant corporations can do hw design

%this empowers individual users to do hardware design, democratizing hardware and compiler design.


%% text from DTR's broader impact statement (NeurIPS, not published) that could be cannibalized here

%Access to increasingly large and expensive accelerators,
%  \textit{e.g.}, the most recent GPUs,
%  has also been a significant force driving
%  DL research for the past decade.
%While such devices have enabled tremendous progress,
%  their expense has also shaped what questions the DL community investigates,
%  which groups are able to conduct such research,
%  and contributed to hazardous technological waste as
%  the previous generation of GPUs quickly becomes obsolete.
%On one hand,
%  we hope this will enable more teams to
%  join the DL research community by eliminating
%  barriers to entry based on
%  limited access to computational resources,
%  and to promote more environmentally friendly hardware reuse.
%On the other hand,
%  any improvement to DL efficiency or applicability
%  may contribute to economic and privacy concerns
%  arising from increased technology company monopolization
%  as discussed in Zuboff's
%  \textit{The Age of Surveillance Capitalism}~\citep{surveillance}.

%We also see opportunities
%  for DTR to build bridges between the DL,
%  Programming Languages (PL), and
%  Computer Architecture (Arch) communities.
%DTR provides a flexible framework where
%  ideas and voices from these communities
%  can collaborate.
%While increased collaboration is an exciting opportunity,
%  there is also the possibility that moving more researchers toward DL
%  will deprive society of significant investigations
%  in other important research areas.

%\clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{bib}

\clearpage
\section*{Broader Impact Statement}

The ability to develop effective compiler support for
  specialized hardware accelerators in ML,
  and HPC more broadly,
  has generally been restricted to a handful of
  elite, well-resourced teams.
This restriction slows hardware development
  and creates barriers to entry for teams in
  less privileged environments to contribute to
  and help guide the development of the field.

We believe that the access pattern abstraction
  and \g's approach to term rewriting for
  improving compiler support for custom
  accelerators will help advance both
  near-term practical and longer-term principled
  approaches to building flexible compiler infrastructure.
In turn, we hope that this infrastructure will
  help contribute to a broader, more diverse, and
  more inclusive community of folks working
  together to build efficient technologies for social
  good.
  %to improve our communities.
  %by developing technologies for social good.
  
Of course, all technology is political and it can
  be difficult to anticipate how future
  researchers and practitioners may apply \g.
While the most obvious consequence of more
  efficient hardware utilization is better
  performance for users and lower environmental
  impact via decreased power consumption,
  it is also possible that access patterns and \g
  would enable the rapid obsoleting of current
  hardware platforms and therefore contribute
  to harmful electronic waste.
This work could also stimulate
  demand for hardware customization by
  removing compiler development--related overheads and
  ultimately lead to higher negative
  environmental impact similar to the
  situation with respect to custom ASICs
  for bitcoin mining~\cite{qin2020bitcoins}.

Also,
  any improvement to ML efficiency or applicability
  may contribute to economic and privacy concerns
  arising from increased technology company monopolization
  as discussed in Zuboff's
  \textit{The Age of Surveillance Capitalism}~\citep{surveillance}.


\end{document}
