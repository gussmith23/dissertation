\chapter{Evaluation}
\label{sec:part1-evaluation}

Thus far, we have described
  how we have applied
  the thesis of this dissertation---%
  that compiler backends
  should be generated
  from formal models of hardware---%
  in the realm of deep learning accelerators.
We first described the difficulties
  in developing compilers
  for deep learning accelerators.
We then described how these difficulties
  motivated the creation of
  3LA: a mostly-automated, end-to-end
  methodology
  for accelerator development.
(Note again that 3LA itself
  is not a contribution of this dissertation.)
We identified a specific problem
  within this domain
  as a problem of interest:
  mapping applications to accelerators.
To address this problem,
  this dissertation
  introduced
  \g,
  a tensor language
  which enables powerful rewriting techniques.
We then integrated \g 
  

This evaluation specifically evaluates
  \g's contribution
  to 3LA
  and to the thesis of this dissertation.
\cref{thesis:optimizations}
\cref{thesis:devtime}
\cref{thesis:correctness}
\hl{todo}

\hl{todo: direct evaluation vs indirect? that is, some of the eval is evaluating non-glenside things, but those things were enabled by glenside. we'd like to include them, because they do help prove the thesis, i.e. the testing results improve correctness. how to be explicit about what's evaluating glenside and what's not}
  

\hl{old:}

In this section, 
  we evaluate our prototype 
  %to evaluate 
  for end-to-end testing with \AppNum applications and three accelerator designs. We focus especially on
  %As part of this we also evaluate 
\begin{inlinelist}
\item automated identification of acceleration opportunities, and 
\item application-level validation using automated co-simulation.   
\end{inlinelist}
%
%\hl{Without the {\TLA} methodology, similar evaluation would require developing extensive bespoke compiler and simulation infrastructure, perhaps via BYOC. 
%We demonstrate that efficient end-to-end evaluation can be facilitated by a modular, extensible workflow with reusable components.}
Note that other tools provide very little automated support (if any)
%no existing automated methodology or tool addresses 
for these two capabilities, thus precluding any head-to-head comparisons. 
%Additionally, the prototype is focused on enabling testing,
  %rather than optimization,
% not intended to be
%   an optimizing compiler,
  %so we do not evaluate the performance
  %of the generated code.
%Besides the two focuses, we 
We also report on operator-level evaluation (accuracy and performance) and FPGA-based deployment.
%Further, {\TLA} is not intended to be a full-fledged compiler, and thus is not being evaluated in the performance of the compiled code or against other compilers.

\input{part 1/evaluation/eval-glenside}


\input{part 1/evaluation/eval-accelerator}
\input{part 1/evaluation/eval-application}
\input{part 1/evaluation/tab-compilation}
\input{part 1/evaluation/eval-compilation}
\input{part 1/evaluation/eval-validation-op}
\input{part 1/evaluation/eval-validation-app}
\input{part 1/evaluation/eval-fpga}


% \subsection{Extensibility}
% How hard to add a new accelerator support.
